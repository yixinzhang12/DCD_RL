{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "authors: Matthew Wilson, Daniele Reda\n",
    "created: 2020/01/14\n",
    "last_updated: 2023/02/08\n",
    "-->\n",
    "\n",
    "\n",
    "## CPSC 533V: Assignment 2 - Tabular Q Learning and DQN (TBD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#  Part 1 [54 pts] Tabular Q-Learning \n",
    "\n",
    "Tabular Q-learning is an RL algorithm for problems with discrete states and discrete actions. The algorithm is described in the class notes, which borrows the summary description from [Section 6.5](http://incompleteideas.net/book/RLbook2018.pdf#page=153) of Richard Sutton's RL book. In the tabular approach, the Q-value is represented as a lookup table. As discussed in class, Q-learning can further be extended to continuous states and discrete actions, leading to the [Atari DQN](https://arxiv.org/abs/1312.5602) / Deep Q-learning algorithm.  However, it is important and informative to first fully understand tabular Q-learning.\n",
    "\n",
    "Informally, Q-learning works as follows: The goal is to learn the optimal Q-function: \n",
    "`Q(s,a)`, which is the *value* of being at state `s` and taking action `a`.  Q tells you how well you expect to do, on average, from here on out, given that you act optimally.  Once the Q function is learned, choosing an optimal action is as simple as looping over all possible actions and choosing the one with the highest Q (optimal action $a^* = \\text{argmax}_a Q(s,a)$).  To learn Q, we initialize it arbitrarily and then iteratively refine it using the Bellman backup equation for Q functions, namely: \n",
    "$Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma \\text{max}_a Q(s', a) - Q(s,a)]$.\n",
    "Here, $r$ is the reward associated with with the transition from state s to s', and $\\alpha$ is a learning rate.\n",
    "\n",
    "In the first part of assignment you will implement tabular Q-learning and apply it to CartPole -- an environment with a **continuous** state space.  To apply the tabular method, you will need to discretize the CartPole state space by dividing the state-space into bins.\n",
    "\n",
    "\n",
    "**Goals:**\n",
    "- to become familiar with python/numpy, as well as using an OpenAI Gym environment\n",
    "- to understand tabular Q-learning, by implementing tabular Q-Learning for \n",
    "  a discretized version of a continuous-state environment, and experimenting with the implementation\n",
    "- (optional) to develop further intuition regarding possible variations of the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Deep reinforcement learning has generated impressive results for board games ([Go][go], [Chess/Shogi][chess]), video games ([Atari][atari], [DOTA2][dota], [StarCraft II][scii]), [and][baoding] [robotic][rubix] [control][anymal] ([of][cassie] [course][mimic] ;)).  RL is beginning to work for an increasing range of tasks and capabilities.  At the same time, there are many [gaping holes][irpan] and [difficulties][amid] in applying these methods. Understanding deep RL is important if you wish to have a good grasp of the modern landscape of control methods.\n",
    "\n",
    "These next several assignments are designed to get you started with deep reinforcement learning, to give you a more close and personal understanding of the methods, and to provide you with a good starting point from which you can branch out into topics of interest. You will implement basic versions of some of the important fundamental algorithms in this space, including Q-learning and policy gradient/search methods.\n",
    "\n",
    "We will only have time to cover a subset of methods and ideas in this space.\n",
    "If you want to dig deeper, we suggest following the links given on the course webpage.  Additionally we draw special attention to the [Sutton book](http://incompleteideas.net/book/RLbook2018.pdf) for RL fundamentals and in depth coverage, and OpenAI's [Spinning Up resources](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html) for a concise intro to RL and deep RL concepts, as well as good comparisons and implementations of modern deep RL algorithms.\n",
    "\n",
    "\n",
    "[atari]: https://arxiv.org/abs/1312.5602\n",
    "[go]: https://deepmind.com/research/case-studies/alphago-the-story-so-far\n",
    "[chess]:https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go \n",
    "[dota]: https://openai.com/blog/openai-five/\n",
    "[scii]: https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning\n",
    "[baoding]: https://bair.berkeley.edu/blog/2019/09/30/deep-dynamics/\n",
    "[rubix]: https://openai.com/blog/solving-rubiks-cube/\n",
    "[cassie]: https://www.cs.ubc.ca/~van/papers/2019-CORL-cassie/index.html\n",
    "[mimic]: https://www.cs.ubc.ca/~van/papers/2018-TOG-deepMimic/index.html\n",
    "[anymal]: https://arxiv.org/abs/1901.08652\n",
    "\n",
    "\n",
    "[irpan]: https://www.alexirpan.com/2018/02/14/rl-hard.html\n",
    "[amid]: http://amid.fish/reproducing-deep-rl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/yixinzhang/Desktop/533/CPSC533V_2024W1/.venv/lib/python3.12/site-packages (2.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: gymnasium in /Users/yixinzhang/Desktop/533/CPSC533V_2024W1/.venv/lib/python3.12/site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/yixinzhang/Desktop/533/CPSC533V_2024W1/.venv/lib/python3.12/site-packages (from gymnasium) (2.1.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/yixinzhang/Desktop/533/CPSC533V_2024W1/.venv/lib/python3.12/site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/yixinzhang/Desktop/533/CPSC533V_2024W1/.venv/lib/python3.12/site-packages (from gymnasium) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/yixinzhang/Desktop/533/CPSC533V_2024W1/.venv/lib/python3.12/site-packages (from gymnasium) (0.0.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# # uncomment if necesary\n",
    "!pip install numpy\n",
    "# !pip install gym\n",
    "# # OR:\n",
    "!pip install gymnasium\n",
    "import time\n",
    "import itertools\n",
    "import numpy as np\n",
    "# import gym\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## [12 pts] Explore the CartPole environment \n",
    "\n",
    "Your first task is to familiarize yourself with the OpenAI gym interface and the [CartPole environment](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py)\n",
    "by writing a simple hand-coded policy to try to solve it.  \n",
    "To begin understanding OpenAI Gym environments, [read this first](https://gymnasium.farama.org/api/env/).) \n",
    "The gym interface is very popular and you will see many algorithm implementations and \n",
    "custom environments that support it.  You may even want to use the API in your course projects, \n",
    "to define a custom environment for a task you want to solve.\n",
    "\n",
    "Note that there were several breaking changes introduced in the past few years to the gym API. Some reference algorithm implementations online might be using the old version:\n",
    "- `obs = env.reset()` ->  `obs, info = env.reset()`\n",
    "- `obs, reward, done, info = env.step(action)` to `obs, reward, terminated, truncated, info = env.step(action)`\n",
    "- `env.render()` no longer accepts the `render_mode` parameter (e.g. human mode where the environment is rendered in a pop-up window, or rgb_array which allows headless conversion to images or videos)\n",
    "\n",
    "\n",
    "Below is some example code that runs a simple random policy.  You are to:\n",
    "- **run the code to see what it does**\n",
    "- **write code that chooses an action based on the observation**.  You will need to learn about the gym API and to read the CartPole documentation to figure out what the `action` and `obs` vectors mean for this environment. \n",
    "Your hand-coded policy can be arbitrary, and it should ideally do better than the random policy.  There is no single correct answer. The goal is to become familiar with `env`s.\n",
    "- **write code to print out the total reward gained by your policy in a single episode run**\n",
    "- **answer the short-response questions below** (see the TODOs for all of this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")  # you can also try LunarLander-v2, but make sure to change it back\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "# To find out what the observations mean, read the CartPole documentation.\n",
    "# Uncomment the lines below, or visit the source file: \n",
    "# https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py\n",
    "\n",
    "#cartpole = env.unwrapped\n",
    "#cartpole?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sum of rewards: 47.0\n"
     ]
    }
   ],
   "source": [
    "# 1.1 [10pts]\n",
    "\n",
    "# runs a single episode and render it.  try running this before editing anything\n",
    "obs, info = env.reset()  # get first obs/state\n",
    "total_reward = 0\n",
    "while True:\n",
    "    # TODO: replace this `action` with something that depends on `obs` \n",
    "    # action = env.action_space.sample()  # random action\n",
    "    action = 0 if obs[2] < 0 else 1\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    env.render()\n",
    "    time.sleep(0.1)  # so it doesn't render too quickly\n",
    "    if terminated | truncated: break\n",
    "env.close()\n",
    "\n",
    "# TODO: print out your total sum of rewards here\n",
    "print(f\"Total sum of rewards: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer the questions below, look at the full [source code here](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/cartpole.py) if you haven't already.\n",
    "\n",
    "**1.2. [2pts] Briefly describe your policy.  What observation information does it use?  What score did you achieve (rough maximum and average)?  And how does it compare to the random policy?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: My policy uses pole angle: if the pole is tilting left, push cart to the left, and vice versa. The total rewards is approximately 31-42. This is slightly higher more stable than the random policy which has total rewards of approximately 9-46. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  [12 pts] Discretize the env\n",
    "\n",
    "Next, we need to discretize CartPole's continuous state space to work for tabular Q-learning.  While this is in part  a contrived usage of tabular methods, given the existence of other approaches that are designed to cope with continuous state-spaces, it is also interesting to consider whether tabular methods can be adapted more directly via discretization of the state into bins. Furthermore, tabular methods are simple, interpretabile, and can be proved to converge, and thus they still remain relevant.\n",
    "\n",
    "Your task is to discretize the state/observation space so that it is compatible with tabular Q-learning.  To do this:\n",
    "- **implement `obs_normalizer` to pass its test**\n",
    "- **implement `get_bins` to pass its test**\n",
    "- **then answer question 2.3**\n",
    "\n",
    "[map]: https://arxiv.org/abs/1504.04909\n",
    "[qd]: https://quality-diversity.github.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 [5 pts for passing test_normed]\n",
    "def obs_normalizer(obs):\n",
    "    \"\"\"Normalize the observations between 0 and 1\n",
    "    \n",
    "    If the observation has extremely large bounds, then clip to a reasonable range before normalizing; \n",
    "    (-2,2) should work.  (It is ok if the solution is specific to CartPole)\n",
    "    \n",
    "    Args:\n",
    "        obs (np.ndarray): shape (4,) containing an observation from CartPole using the bound of the env\n",
    "    Returns:\n",
    "        normed (np.ndarray): shape (4,) where all elements are roughly uniformly mapped to the range [0, 1]\n",
    "    \n",
    "    \"\"\"\n",
    "    # HINT: check out env.observation_space.high, env.observation_space.low\n",
    "    \n",
    "    # TODO: implement this function\n",
    "    high = env.observation_space.high\n",
    "    low = env.observation_space.low\n",
    "\n",
    "    # replace with reasonable numbers\n",
    "    high[1] = 2\n",
    "    high[3] = 2\n",
    "    low[1] = -2\n",
    "    low[3] = -2\n",
    "\n",
    "    clipped_obs = np.clip(obs, low, high)\n",
    "    normed = (clipped_obs - low) / (high - low)\n",
    "    return normed\n",
    "    # raise NotImplementedError('TODO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed!\n"
     ]
    }
   ],
   "source": [
    "### TEST 2.1\n",
    "def test_normed():\n",
    "    obs, info = env.reset()\n",
    "    while True:\n",
    "        obs, _, terminated, truncated, _ =  env.step(env.action_space.sample())\n",
    "        normed = obs_normalizer(obs) \n",
    "        assert np.all(normed >= 0.0) and np.all(normed <= 1.0), '{} are outside of (0,1)'.format(normed)\n",
    "        if terminated | truncated: break\n",
    "    env.close()\n",
    "    print('Passed!')\n",
    "test_normed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 [5 pts for passing test_binned]\n",
    "def get_bins(normed, num_bins):\n",
    "    \"\"\"Map normalized observations (0,1) to bin index values (0,num_bins-1)\n",
    "    \n",
    "    Args:\n",
    "        normed (np.ndarray): shape (4,) output from obs_normalizer\n",
    "        num_bins (int): how many bins to use\n",
    "    Returns:\n",
    "        binned (np.ndarray of type np.int32): shape (4,) where all elements are values in range [0,num_bins-1]\n",
    "    \n",
    "    \"\"\"\n",
    "    scaled = normed * (num_bins-1)\n",
    "    binned = np.floor(scaled).astype(np.int32)    \n",
    "\n",
    "    return binned  \n",
    "    # TODO: implement this function\n",
    "    # raise NotImplementedError('TODO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed!\n"
     ]
    }
   ],
   "source": [
    "### TEST 2.2\n",
    "obs, info = env.reset()\n",
    "\n",
    "def test_binned(num_bins):\n",
    "    normed = np.array([0.0, 0.2, 0.8, 1.0])\n",
    "    binned = get_bins(normed, num_bins)\n",
    "    assert np.all(binned >= 0) and np.all(binned < num_bins), '{} supposed to be between (0, {})'.format(binned, num_bins-1)\n",
    "    assert binned.dtype == np.int32, \"You should also make sure to cast your answer to int using arr.astype(np.int32)\" \n",
    "    \n",
    "test_binned(5)\n",
    "test_binned(10)\n",
    "test_binned(50)\n",
    "print('Passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3. [2 pts] If your state has 4 values and each is binned into N possible bins, how many bins are needed to represent all unique possible states?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: $N^4$ bins are needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## [20 pts] Solve the env \n",
    "\n",
    "Using the pseudocode below and the functions you implemented above, implement tabular Q-learning and use it to solve CartPole.\n",
    "\n",
    "We provide setup code to initialize the Q-table and give examples of interfacing with it. Write the inner and outer loops to train your algorithm.  These training loops will be similar to those deep RL approaches, so get used to writing them!\n",
    "\n",
    "The algorithm (excerpted from Section 6.5 of [Sutton's book](http://incompleteideas.net/book/RLbook2018.pdf)) is given below:\n",
    "\n",
    "![Sutton RL](https://i.imgur.com/mdcWVRL.png)\n",
    "\n",
    "in summary:\n",
    "- **implement Q-learning using this pseudocode and the helper code**\n",
    "- **answer the questions below**\n",
    "- **run the suggested experiments and otherwise experiment with whatever interests you**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Q Table:  (10, 10, 10, 10, 2)\n",
      "Original obs [ 0.0222021  -0.03588101  0.00441333  0.04630226] --> binned (np.int32(4), np.int32(4), np.int32(4), np.int32(4))\n",
      "Value of Q Table at that obs/state value [0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# setup (see last few lines for how to use the Q-table)\n",
    "\n",
    "# hyper parameters. feel free to change these as desired and experiment with different values\n",
    "num_bins = 10\n",
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "log_n = 1000\n",
    "# epsilon greedy\n",
    "eps = 0.05  #usage: action = optimal if np.random.rand() > eps else random\n",
    "\n",
    "obs, info = env.reset()\n",
    "\n",
    "# Q-table initialized to zeros.  first 4 dims are state, last dim is for action (0,1) for left,right.\n",
    "Q = np.zeros([num_bins]*len(obs)+[env.action_space.n])\n",
    "\n",
    "# helper function to convert observation into a binned state so we can index into our Q-table\n",
    "obs2bin = lambda obs: tuple(get_bins(obs_normalizer(obs), num_bins=num_bins))\n",
    "\n",
    "s = obs2bin(obs)\n",
    "\n",
    "print('Shape of Q Table: ', Q.shape) # you can imagine why tabular learning does not scale very well\n",
    "print('Original obs {} --> binned {}'.format(obs, s))\n",
    "print('Value of Q Table at that obs/state value', Q[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 [20 pts]\n",
    "\n",
    "# TODO: implement Q learning, following the pseudo-code above. \n",
    "#     - you can follow it almost exactly, but translating things for the gym api and our code used above\n",
    "#     - make sure to use e-greedy, where e = random about 0.05 percent of the time\n",
    "#     - make sure to do the S <-- S' step because it can be easy to forget\n",
    "#     - every log_n steps, you should render your environment and\n",
    "#       print out the average total episode rewards of the past log_n runs to monitor how your agent trains\n",
    "#      (your implementation should be able to break at least +150 average reward value, and you can use that \n",
    "#       as a breaking condition.  It make take several minutes to run depending on your computer.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(env, Q, num_episode, log_n, num_bins, alpha, gamma, eps):\n",
    "    obs, info = env.reset()\n",
    "    obs2bin = lambda obs: tuple(get_bins(obs_normalizer(obs), num_bins=num_bins))\n",
    "    s = obs2bin(obs)\n",
    "    rewards_per_episode = []\n",
    "    for episode in range(num_episode):\n",
    "        obs, info = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        s = obs2bin(obs) # discretize\n",
    "\n",
    "        while done == False:\n",
    "            # Epsilon-greedy\n",
    "            if np.random.rand() > eps:\n",
    "                a = np.argmax(Q[s])\n",
    "            else:\n",
    "                a = env.action_space.sample()\n",
    "\n",
    "            # take action, observe R and S'\n",
    "            obs_new, reward, terminated, truncated, info = env.step(a)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            s_new = obs2bin(obs_new)\n",
    "\n",
    "            # update q = current q + max future q\n",
    "            max_future_q = np.max(Q[s_new])\n",
    "            current_q = Q[s + (a,)]\n",
    "            new_q = current_q + alpha * (reward + gamma * max_future_q - current_q)\n",
    "            Q[s + (a,)] = new_q\n",
    "\n",
    "            # update state and reward\n",
    "            s = s_new\n",
    "            total_reward += reward\n",
    "\n",
    "        rewards_per_episode.append(total_reward)\n",
    "\n",
    "        # Logging every log_n episodes\n",
    "        if (episode+1) % log_n == 0:\n",
    "            avg_reward_log = np.mean(rewards_per_episode[-log_n:])\n",
    "            print(f\"Episode {episode + 1}/{num_episode}, Average Reward: {avg_reward_log}\")\n",
    "            # Break if average reward >= 150\n",
    "            if avg_reward_log >= 150:\n",
    "                print(f\"Breaking Criterion Reached: Average reward reached {avg_reward_log} after {episode + 1} episodes.\")\n",
    "                break\n",
    "\n",
    "\n",
    "    avg_reward = np.mean(rewards_per_episode)\n",
    "    print(f\"Average Reward after {num_episode} episodes: {avg_reward}\")\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000/30000, Average Reward: 12.123\n",
      "Episode 2000/30000, Average Reward: 31.068\n",
      "Episode 3000/30000, Average Reward: 59.122\n",
      "Episode 4000/30000, Average Reward: 99.834\n",
      "Episode 5000/30000, Average Reward: 136.916\n",
      "Episode 6000/30000, Average Reward: 174.617\n",
      "Breaking Criterion Reached: Average reward reached 174.617 after 6000 episodes.\n",
      "Average Reward after 30000 episodes: 85.61333333333333\n"
     ]
    }
   ],
   "source": [
    "Q_learning(env, Q = np.zeros([num_bins]*len(obs)+[env.action_space.n]), num_episode=30000, log_n=1000, num_bins = 10, alpha=0.1, gamma=0.99, eps=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10 pts] Experiments\n",
    "\n",
    "Given a working algorithm, you will run a few experiments.  Either make a copy of your code above to modify, or make the modifications in a way that they can be commented out or switched between (with boolean flag if statements)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2. [5 pts] $\\epsilon$-greedy.**  How sensitive are the results to the value of $\\epsilon$?   First, write down your prediction of what would happen if $\\epsilon$ is set to various values, including for example [0, 0.05, 0.25, 0.5]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: The results should be sensitive to $\\epsilon$. $\\epsilon = 0$ should be exploiting and have a low total reward due to lack of exploration. $\\epsilon = 0.05$ should have higher max reward than $\\epsilon = 0$ and $\\epsilon = 0.25$ should have higher rewards than $\\epsilon = 0.05$. However, $\\epsilon = 0.5$ may over explore and unable to exploit on a optimal q table, therefore, taking longer to have higher rewards than $\\epsilon = 0.05$ and $\\epsilon = 0.25$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the experiment and observe the impact on the algorithm.  Report the results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000/30000, Average Reward: 9.364\n",
      "Episode 2000/30000, Average Reward: 9.353\n",
      "Episode 3000/30000, Average Reward: 9.362\n",
      "Episode 4000/30000, Average Reward: 9.354\n",
      "Episode 5000/30000, Average Reward: 9.375\n",
      "Episode 6000/30000, Average Reward: 9.352\n",
      "Episode 7000/30000, Average Reward: 9.377\n",
      "Episode 8000/30000, Average Reward: 9.386\n",
      "Episode 9000/30000, Average Reward: 9.361\n",
      "Episode 10000/30000, Average Reward: 9.357\n",
      "Episode 11000/30000, Average Reward: 9.372\n",
      "Episode 12000/30000, Average Reward: 9.346\n",
      "Episode 13000/30000, Average Reward: 9.386\n",
      "Episode 14000/30000, Average Reward: 9.376\n",
      "Episode 15000/30000, Average Reward: 9.395\n",
      "Episode 16000/30000, Average Reward: 9.333\n",
      "Episode 17000/30000, Average Reward: 9.38\n",
      "Episode 18000/30000, Average Reward: 9.377\n",
      "Episode 19000/30000, Average Reward: 9.303\n",
      "Episode 20000/30000, Average Reward: 9.394\n",
      "Episode 21000/30000, Average Reward: 9.373\n",
      "Episode 22000/30000, Average Reward: 9.339\n",
      "Episode 23000/30000, Average Reward: 9.337\n",
      "Episode 24000/30000, Average Reward: 9.372\n",
      "Episode 25000/30000, Average Reward: 9.326\n",
      "Episode 26000/30000, Average Reward: 9.294\n",
      "Episode 27000/30000, Average Reward: 9.379\n",
      "Episode 28000/30000, Average Reward: 9.404\n",
      "Episode 29000/30000, Average Reward: 9.311\n",
      "Episode 30000/30000, Average Reward: 9.341\n",
      "Average Reward after 30000 episodes: 9.3593\n",
      "Episode 1000/30000, Average Reward: 26.072\n",
      "Episode 2000/30000, Average Reward: 84.386\n",
      "Episode 3000/30000, Average Reward: 85.909\n",
      "Episode 4000/30000, Average Reward: 95.851\n",
      "Episode 5000/30000, Average Reward: 87.742\n",
      "Episode 6000/30000, Average Reward: 89.84\n",
      "Episode 7000/30000, Average Reward: 86.118\n",
      "Episode 8000/30000, Average Reward: 91.398\n",
      "Episode 9000/30000, Average Reward: 85.811\n",
      "Episode 10000/30000, Average Reward: 82.293\n",
      "Episode 11000/30000, Average Reward: 89.139\n",
      "Episode 12000/30000, Average Reward: 95.7\n",
      "Episode 13000/30000, Average Reward: 86.276\n",
      "Episode 14000/30000, Average Reward: 88.762\n",
      "Episode 15000/30000, Average Reward: 82.031\n",
      "Episode 16000/30000, Average Reward: 82.308\n",
      "Episode 17000/30000, Average Reward: 84.21\n",
      "Episode 18000/30000, Average Reward: 85.043\n",
      "Episode 19000/30000, Average Reward: 85.658\n",
      "Episode 20000/30000, Average Reward: 89.202\n",
      "Episode 21000/30000, Average Reward: 83.3\n",
      "Episode 22000/30000, Average Reward: 80.9\n",
      "Episode 23000/30000, Average Reward: 83.589\n",
      "Episode 24000/30000, Average Reward: 89.78\n",
      "Episode 25000/30000, Average Reward: 85.337\n",
      "Episode 26000/30000, Average Reward: 87.154\n",
      "Episode 27000/30000, Average Reward: 86.887\n",
      "Episode 28000/30000, Average Reward: 91.043\n",
      "Episode 29000/30000, Average Reward: 86.072\n",
      "Episode 30000/30000, Average Reward: 72.07\n",
      "Average Reward after 30000 episodes: 84.32936666666667\n",
      "Episode 1000/30000, Average Reward: 48.613\n",
      "Episode 2000/30000, Average Reward: 68.564\n",
      "Episode 3000/30000, Average Reward: 66.167\n",
      "Episode 4000/30000, Average Reward: 65.408\n",
      "Episode 5000/30000, Average Reward: 69.067\n",
      "Episode 6000/30000, Average Reward: 66.715\n",
      "Episode 7000/30000, Average Reward: 67.309\n",
      "Episode 8000/30000, Average Reward: 68.628\n",
      "Episode 9000/30000, Average Reward: 64.58\n",
      "Episode 10000/30000, Average Reward: 67.886\n",
      "Episode 11000/30000, Average Reward: 73.954\n",
      "Episode 12000/30000, Average Reward: 66.675\n",
      "Episode 13000/30000, Average Reward: 68.859\n",
      "Episode 14000/30000, Average Reward: 65.739\n",
      "Episode 15000/30000, Average Reward: 68.68\n",
      "Episode 16000/30000, Average Reward: 66.888\n",
      "Episode 17000/30000, Average Reward: 63.196\n",
      "Episode 18000/30000, Average Reward: 69.848\n",
      "Episode 19000/30000, Average Reward: 64.525\n",
      "Episode 20000/30000, Average Reward: 61.524\n",
      "Episode 21000/30000, Average Reward: 63.755\n",
      "Episode 22000/30000, Average Reward: 63.644\n",
      "Episode 23000/30000, Average Reward: 63.872\n",
      "Episode 24000/30000, Average Reward: 67.258\n",
      "Episode 25000/30000, Average Reward: 64.009\n",
      "Episode 26000/30000, Average Reward: 59.843\n",
      "Episode 27000/30000, Average Reward: 64.267\n",
      "Episode 28000/30000, Average Reward: 63.148\n",
      "Episode 29000/30000, Average Reward: 66.039\n",
      "Episode 30000/30000, Average Reward: 63.904\n",
      "Average Reward after 30000 episodes: 65.4188\n"
     ]
    }
   ],
   "source": [
    "# explore epsilons\n",
    "Q_learning(env, Q = np.zeros([num_bins]*len(obs)+[env.action_space.n]), num_episode=30000, log_n=1000, num_bins = 10, alpha=0.1, gamma=0.99, eps=0)\n",
    "Q_learning(env, Q = np.zeros([num_bins]*len(obs)+[env.action_space.n]), num_episode=30000, log_n=1000, num_bins = 10, alpha=0.1, gamma=0.99, eps=0.25)\n",
    "Q_learning(env, Q = np.zeros([num_bins]*len(obs)+[env.action_space.n]), num_episode=30000, log_n=1000, num_bins = 10, alpha=0.1, gamma=0.99, eps=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**: Average rewrad after 10000 episodes is the highest with epsilon = 0.25 with all else being equal, matching the prediction above. epsilon = 0 fail to train and epsilon = 0.5 improves slowest due to lack of exploration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3. [5 pts] Design your own experiment.** Design a modification that you think would either increase or reduce performance.  A simple example (which you can use) is initializing the Q-table differently, and thinking about how this might alter performance. Write down your idea, what you think might happen, and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: I plan to initialize Q-table to a higher value than 0, which would improve exploration at the beginning since all other unexplored values have more optimal rewards. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the experiment and report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000/30000, Average Reward: 13.898\n",
      "Episode 2000/30000, Average Reward: 55.903\n",
      "Episode 3000/30000, Average Reward: 92.274\n",
      "Episode 4000/30000, Average Reward: 100.075\n",
      "Episode 5000/30000, Average Reward: 101.244\n",
      "Episode 6000/30000, Average Reward: 109.923\n",
      "Episode 7000/30000, Average Reward: 107.458\n",
      "Episode 8000/30000, Average Reward: 106.927\n",
      "Episode 9000/30000, Average Reward: 111.902\n",
      "Episode 10000/30000, Average Reward: 110.294\n",
      "Episode 11000/30000, Average Reward: 115.405\n",
      "Episode 12000/30000, Average Reward: 128.007\n",
      "Episode 13000/30000, Average Reward: 120.954\n",
      "Episode 14000/30000, Average Reward: 136.476\n",
      "Episode 15000/30000, Average Reward: 126.011\n",
      "Episode 16000/30000, Average Reward: 125.913\n",
      "Episode 17000/30000, Average Reward: 124.253\n",
      "Episode 18000/30000, Average Reward: 123.835\n",
      "Episode 19000/30000, Average Reward: 137.963\n",
      "Episode 20000/30000, Average Reward: 141.518\n",
      "Episode 21000/30000, Average Reward: 128.065\n",
      "Episode 22000/30000, Average Reward: 121.603\n",
      "Episode 23000/30000, Average Reward: 116.015\n",
      "Episode 24000/30000, Average Reward: 110.88\n",
      "Episode 25000/30000, Average Reward: 106.741\n",
      "Episode 26000/30000, Average Reward: 90.206\n",
      "Episode 27000/30000, Average Reward: 75.69\n",
      "Episode 28000/30000, Average Reward: 81.284\n",
      "Episode 29000/30000, Average Reward: 54.156\n",
      "Episode 30000/30000, Average Reward: 42.562\n",
      "Average Reward after 30000 episodes: 103.9145\n",
      "Episode 1000/30000, Average Reward: 10.899\n",
      "Episode 2000/30000, Average Reward: 10.837\n",
      "Episode 3000/30000, Average Reward: 10.883\n",
      "Episode 4000/30000, Average Reward: 10.834\n",
      "Episode 5000/30000, Average Reward: 10.803\n",
      "Episode 6000/30000, Average Reward: 10.956\n",
      "Episode 7000/30000, Average Reward: 10.899\n",
      "Episode 8000/30000, Average Reward: 10.878\n",
      "Episode 9000/30000, Average Reward: 10.903\n",
      "Episode 10000/30000, Average Reward: 10.963\n",
      "Episode 11000/30000, Average Reward: 10.824\n",
      "Episode 12000/30000, Average Reward: 11.078\n",
      "Episode 13000/30000, Average Reward: 10.854\n",
      "Episode 14000/30000, Average Reward: 10.926\n",
      "Episode 15000/30000, Average Reward: 10.788\n",
      "Episode 16000/30000, Average Reward: 10.756\n",
      "Episode 17000/30000, Average Reward: 10.84\n",
      "Episode 18000/30000, Average Reward: 10.907\n",
      "Episode 19000/30000, Average Reward: 10.837\n",
      "Episode 20000/30000, Average Reward: 10.869\n",
      "Episode 21000/30000, Average Reward: 10.984\n",
      "Episode 22000/30000, Average Reward: 10.91\n",
      "Episode 23000/30000, Average Reward: 10.804\n",
      "Episode 24000/30000, Average Reward: 10.921\n",
      "Episode 25000/30000, Average Reward: 10.902\n",
      "Episode 26000/30000, Average Reward: 10.833\n",
      "Episode 27000/30000, Average Reward: 10.77\n",
      "Episode 28000/30000, Average Reward: 10.947\n",
      "Episode 29000/30000, Average Reward: 10.832\n",
      "Episode 30000/30000, Average Reward: 10.87\n",
      "Average Reward after 30000 episodes: 10.8769\n",
      "Episode 1000/30000, Average Reward: 10.948\n",
      "Episode 2000/30000, Average Reward: 10.869\n",
      "Episode 3000/30000, Average Reward: 10.942\n",
      "Episode 4000/30000, Average Reward: 10.953\n",
      "Episode 5000/30000, Average Reward: 10.831\n",
      "Episode 6000/30000, Average Reward: 10.871\n",
      "Episode 7000/30000, Average Reward: 10.908\n",
      "Episode 8000/30000, Average Reward: 10.995\n",
      "Episode 9000/30000, Average Reward: 10.83\n",
      "Episode 10000/30000, Average Reward: 10.843\n",
      "Episode 11000/30000, Average Reward: 11.016\n",
      "Episode 12000/30000, Average Reward: 10.79\n",
      "Episode 13000/30000, Average Reward: 10.91\n",
      "Episode 14000/30000, Average Reward: 10.961\n",
      "Episode 15000/30000, Average Reward: 10.763\n",
      "Episode 16000/30000, Average Reward: 10.859\n",
      "Episode 17000/30000, Average Reward: 10.849\n",
      "Episode 18000/30000, Average Reward: 10.873\n",
      "Episode 19000/30000, Average Reward: 10.854\n",
      "Episode 20000/30000, Average Reward: 10.884\n",
      "Episode 21000/30000, Average Reward: 10.947\n",
      "Episode 22000/30000, Average Reward: 10.935\n",
      "Episode 23000/30000, Average Reward: 11.077\n",
      "Episode 24000/30000, Average Reward: 10.902\n",
      "Episode 25000/30000, Average Reward: 10.948\n",
      "Episode 26000/30000, Average Reward: 10.876\n",
      "Episode 27000/30000, Average Reward: 10.848\n",
      "Episode 28000/30000, Average Reward: 10.948\n",
      "Episode 29000/30000, Average Reward: 10.897\n",
      "Episode 30000/30000, Average Reward: 10.702\n",
      "Average Reward after 30000 episodes: 10.8943\n"
     ]
    }
   ],
   "source": [
    "# explore Q table initialization\n",
    "Q_learning(env, Q = np.ones([num_bins]*len(obs)+[env.action_space.n]), num_bins = 10, num_episode=30000, log_n=1000, alpha=0.1, gamma=0.99, eps=0.25)\n",
    "Q_learning(env, Q = np.full([num_bins]*len(obs)+[env.action_space.n], 2), num_bins = 10, num_episode=30000, log_n=1000, alpha=0.1, gamma=0.99, eps=0.25)\n",
    "Q_learning(env, Q = np.full([num_bins]*len(obs)+[env.action_space.n], 5), num_bins = 10, num_episode=30000, log_n=1000, alpha=0.1, gamma=0.99, eps=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Initiaing Q table to ones would have similar effect as setting Q table to zeros and learn slightly faster in some runs. However, any higher initial values (2 and 5) would potentially overestimate rewards and fail to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## A. Extensions (fully optional, will not be graded, if you have time after Part 2)\n",
    "\n",
    "- plots your learning curve, using e.g., matplotlib \n",
    "- visualize the Q-table to see which values are being updated and not\n",
    "- design a better binning strategy that uses fewer bins for a better-performing policy\n",
    "- extend this approach to work on different environments (e.g., LunarLander-v2)\n",
    "- extend this approach to work on environments with continuous actions, by using a fixed set of discrete samples of the action space.  e.g., for Pendulum-v0\n",
    "- implement a simple deep learning version of this.  we will see next part that DQN uses some tricks to make the neural network training more stable.  Experiment directly with simply replacing the Q-table with a Q-Network and train the Q-Network using gradient descent with `loss = (targets - Q(s,a))**2`, where `targets = stop_grad(R + gamma * maxa(Q(s,a))`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 [60 pts] Behavioral Cloning and Deep Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The second part of assignment will help you transition from tabular approaches to deep neural network approaches. You will implement the [Atari DQN / Deep Q-Learning](https://arxiv.org/abs/1312.5602) algorithm, which arguably kicked off the modern Deep Reinforcement Learning craze.\n",
    "\n",
    "In this part we will use PyTorch as our deep learning framework.  To familiarize yourself with PyTorch, your first task is to use a behavior cloning (BC) approach to learn a policy.  Behavior cloning is a supervised learning method in which there exists a dataset of expert demonstrations (state-action pairs) and the goal is to learn a policy $\\pi$ that mimics this expert.  At any given state, your policy should choose the same action the export would.\n",
    "\n",
    "Since BC avoids the need to collect data from the policy you are trying to learn, it is relatively simple. \n",
    "This makes it a nice stepping stone for implementing DQN. Furthermore, BC is relevant to modern approaches---for example its use as an initialization for systems like [AlphaGo][go] and [AlphaStar][star], which then use RL to further adapte the BC result.  \n",
    "\n",
    "<!--\n",
    "\n",
    "I feel like this might be better suited to going lower in the document:\n",
    "\n",
    "Unfortunately, in many tasks it is impossible to collect good expert demonstrations, making\n",
    "\n",
    "it's not always possible to have good expert demonstrations for a task in an environemnt and this is where reinforcement learning comes handy. Through the reward signal retrieved by interacting with the environment, the agent learns by itself what is a good policy and can learn to outperform the experts.\n",
    "\n",
    "-->\n",
    "\n",
    "Goals:\n",
    "- Famliarize yourself with PyTorch and its API including models, datasets, dataloaders\n",
    "- Implement a supervised learning approach (behavioral cloning) to learn a policy.\n",
    "- Implement the DQN objective and learn a policy through environment interaction.\n",
    "\n",
    "[go]:  https://deepmind.com/research/case-studies/alphago-the-story-so-far\n",
    "[star]: https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii\n",
    "\n",
    "## Submission information\n",
    "\n",
    "- Complete by editing and executing the associated Python files.\n",
    "- Copy and paste the code and the terminal output requested in the predefined cells on this Jupyter notebook.\n",
    "- When done, upload the completed Jupyter notebook (ipynb file) on canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "### PyTorch\n",
    "\n",
    "If you have never used PyTorch before, we recommend you follow this [60 Minutes Blitz][blitz] tutorial from the official website. It should give you enough context to be able to complete the assignment.\n",
    "\n",
    "\n",
    "**If you have issues, post questions to Piazza**\n",
    "\n",
    "### Installation\n",
    "\n",
    "To install all required python packages:\n",
    "\n",
    "```\n",
    "python3 -m pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### Debugging\n",
    "\n",
    "\n",
    "You can include:  `import ipdb; ipdb.set_trace()` in your code and it will drop you to that point in the code, where you can interact with variables and test out expressions.  We recommend this as an effective method to debug the algorithms.\n",
    "\n",
    "\n",
    "[blitz]: https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. [36 pts] Behavioral Cloning\n",
    "\n",
    "Behavioral Cloning is a type of supervised learning in which you are given a dataset of expert demonstrations tuple $(s, a)$ and the goal is to learn a policy function $\\hat a = \\pi(s)$, such that $\\hat a = a$.\n",
    "\n",
    "The optimization objective is $\\min_\\theta D(\\pi(s), a)$ where $\\theta$ are the parameters the policy $\\pi$, in our case the weights of a neural network, and where $D$ represents some difference between the actions.\n",
    "\n",
    "---\n",
    "\n",
    "Before starting, we suggest reading through the provided files.\n",
    "\n",
    "For Behavioral Cloning, the important files to understand are: `model.py`, `dataset.py` and `bc.py`.\n",
    "\n",
    "- The file `model.py` has the skeleton for the model (which you will have to complete in the following questions),\n",
    "\n",
    "- The file `dataset.py` has the skeleton for the dataset the model is being trained with,\n",
    "\n",
    "- and, `bc.py` will have all the structure for training the model with the dataset.\n",
    "\n",
    "\n",
    "### [10 pts] 1.1 Dataset\n",
    "\n",
    "We provide a pickle file with pre-collected expert demonstrations on CartPole from which to learn the policy $\\pi$. The data has been collected from an expert policy on the environment, with the addition of a small amount of gaussian noise to the actions.\n",
    "\n",
    "The pickle file contains a list of tuples of states and actions in `numpy` in the following way:\n",
    "\n",
    "```\n",
    "[(state s, action a), (state s, action a), (state s, action a), ...]\n",
    "```\n",
    "\n",
    "In the `dataset.py` file, we provide skeleton code for creating a custom dataset. The provided code shows how to load the file.\n",
    "\n",
    "Your goal is to overwrite the `__getitem__` function in order to return a dictionary of tensors of the correct type.\n",
    "\n",
    "Hint: Look in the `bc.py` file to understand how the dataset is used.\n",
    "\n",
    "Answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[6 pts]** Insert your code in the placeholder below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLACEHOLDER TO INSERT YOUR __getitem__ method here\n",
    "\n",
    "# def __getitem__(self, index):\n",
    "#     item = self.data[index]\n",
    "#     # TODO YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "\n",
    "### Answer ###\n",
    "# def __getitem__(self, index):\n",
    "#     item = self.data[index]\n",
    "#     state_np, action_np = item\n",
    "#     state = torch.tensor(state_np, dtype=torch.float32)\n",
    "#     action = torch.tensor(action_np, dtype=torch.long)\n",
    "#     return {'state': state, 'action': action} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 99660\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(data_path=\"CartPole-v1_dataset.pkl\")\n",
    "dataset_size = len(dataset)\n",
    "print(f\"Dataset size: {dataset_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The state has 4 dimensiosn.\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[0]\n",
    "state = sample['state']\n",
    "state_dim = state.shape[0]\n",
    "print(f\"The state has {state_dim} dimensiosn.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max = tensor([2.3995, 1.8470, 0.1464, 0.4714])\n",
      "min = tensor([-0.7227, -0.4330, -0.0501, -0.3812])\n"
     ]
    }
   ],
   "source": [
    "all_states = torch.stack([dataset[i]['state'] for i in range(len(dataset))])\n",
    "state_min = torch.min(all_states, dim=0)[0]\n",
    "state_max = torch.max(all_states, dim=0)[0]\n",
    "print(f\"max = {state_max}\")\n",
    "print(f\"min = {state_min}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action has dimension torch.Size([99660]).\n",
      "Unique actions in dataset: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "all_actions = torch.tensor([dataset[i]['action'] for i in range(len(dataset))])\n",
    "actions_unique = torch.unique(all_actions)\n",
    "print(f\"Action has dimension {all_actions.shape}.\")\n",
    "print(f\"Unique actions in dataset: {actions_unique.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 pt]** How big is the dataset provided?\n",
    "\n",
    "**Answer:** dataset contains 99660 samples.\n",
    "\n",
    "**[2 pts]** What is the dimensionality of $s$ and what range does each dimension of $s$ span?  I.e., how much of the state space does the expert data cover? What are the dimensionalities and ranges of the action $a$ in the dataset (how much of the action space does the expert data cover)?\n",
    "\n",
    "**Answer:** $s$ has 4 dimensions, dim0 ranges from -0.7227 to 2.3995, dim1 ranges from -0.4330 to 1.8470, dim2 ranges from -0.0501 to 0.1464, and dim3 ranges from -0.3812 to 0.4714. $a$ is a scalar and is either 0 or 1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 pts] 1.2 Environment\n",
    "\n",
    "Recall the state and action space of CartPole, from the previous assignment.\n",
    "\n",
    "Considering the full state and action spaces, do you think the provided expert dataset has good coverage?  Why or why not? How might this impact the performance of our cloned policy?\n",
    "\n",
    "**Answer:** No. The provided range covers a narrow portion (around center) of the full range. In cases where the state is outside of this range, the cloned policy may not perform well because it lacks corresponding data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [14 pts] 1.3 Model\n",
    "\n",
    "The file `model.py` provides skeleton code for the model. Your goal is to create the architecture of the network by adding layers that map the input to output.\n",
    "\n",
    "You will need to update the `__init__` method and the `forward` method.\n",
    "\n",
    "The `select_action` method has already been written for you.  This should be used when running the policy in the environment, while the `forward` function should be used at training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **[10 pts]** Insert your code in the placeholder below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLACEHOLDER TO INSERT YOUR MyModel class here\n",
    "\n",
    "# class MyModel(nn.Module):\n",
    "#     def __init__(self, state_size, action_size):\n",
    "#         super(MyModel, self).__init__()\n",
    "#         # TODO YOUR CODE HERE FOR INITIALIZING THE MODEL\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # TODO YOUR CODE HERE FOR THE FORWARD PASS\n",
    "#         raise NotImplementedError()\n",
    "\n",
    "#     def select_action(self, state):\n",
    "#         self.eval()\n",
    "#         x = self.forward(state)\n",
    "#         self.train()\n",
    "#         return x.max(1)[1].view(1, 1).to(torch.long)\n",
    "\n",
    "\n",
    "# ###Answer###\n",
    "# class MyModel(nn.Module):\n",
    "#     def __init__(self, state_size, action_size):\n",
    "#         super().__init__()\n",
    "#         self.fc1 = nn.Linear(state_size, 32) \n",
    "#         self.fc2 = nn.Linear(32, 32)\n",
    "#         self.fc3 = nn.Linear(32, action_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "#     def select_action(self, state):\n",
    "#         self.eval()\n",
    "#         x = self.forward(state) \n",
    "#         self.train()\n",
    "#         return x.max(1)[1].view(1, 1).to(torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions:\n",
    "\n",
    "- **[2 pts]** What is the dimension and meaning of the input of the network\n",
    "\n",
    "**Answer:** The input would have 4 dimensions, each corresponding to the state (cart position, cart velocity, pole angle, pole angular velocity). \n",
    "\n",
    "- **[2 pts]** Similarly, describe the output.\n",
    "\n",
    "**Answer:** The output would have 2 dimensions, each corresponding to the probability of taking one action (0 for pushing to left, 1 for pushing to right).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [7 pts] 1.4 Training\n",
    "\n",
    "The file `bc.py` is the entry point for training your behavioral cloning model. The skeleton and the main components are already there.\n",
    "\n",
    "The missing parts for you to do are:\n",
    "\n",
    "- Initializing the model\n",
    "- Choosing a loss function\n",
    "- Choosing an optimizer\n",
    "- Playing with hyperparameters to train your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **[5 pts]** Insert your code in the placeholder below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLACEHOLDER FOR YOUR CODE HER\n",
    "# HOW DID YOU INITIALIZE YOUR MODEL, OPTIMIZER AND LOSS FUNCTIONS? PASTE HERE YOUR FINAL CODE\n",
    "# NOTE: YOU CAN KEEP THE FOLLOWING LINES COMMENTED OUT, AS RUNNING THIS CELL WILL PROBABLY RESULT IN ERRORS\n",
    "\n",
    "# model = None\n",
    "# optimizer = None\n",
    "# loss_function = None\n",
    "\n",
    "###Answer###\n",
    "# state_size = env.observation_space.shape[0]  \n",
    "# action_size = env.action_space.n \n",
    "# model = MyModel(state_size, action_size).to(device)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)  \n",
    "# loss_function = nn.CrossEntropyLoss()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run your code by doing:\n",
    "\n",
    "```\n",
    "python3 bc.py\n",
    "```\n",
    "\n",
    "**During all of this assignment, the code in `eval_policy.py` will be your best friend.** At any time, you can test your model by giving as argument the path to the model weights and the environment name using the following command:\n",
    "\n",
    "```\n",
    "python3 eval_policy.py --model-path /path/to/model/weights --env ENV_NAME\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PASTE YOUR TERMINAL OUTPUT HERE\n",
    "# NOTE: TO HAVE LESS LINES PRINTED, YOU CAN SET THE VARIABLE PRINT_INTERVAL TO 5 or 10\n",
    "\n",
    "# [epoch    1/100] [iter       0] [loss 0.70570]\n",
    "# [Test on environment] [epoch 2/100] [score 306.90]\n",
    "# [Test on environment] [epoch 4/100] [score 248.80]\n",
    "# [Test on environment] [epoch 6/100] [score 260.50]\n",
    "# [epoch    7/100] [iter   10000] [loss 0.01669]\n",
    "# [Test on environment] [epoch 8/100] [score 262.70]\n",
    "# [Test on environment] [epoch 10/100] [score 248.90]\n",
    "# [Test on environment] [epoch 12/100] [score 222.70]\n",
    "# [epoch   13/100] [iter   20000] [loss 0.00198]\n",
    "# [Test on environment] [epoch 14/100] [score 271.40]\n",
    "# [Test on environment] [epoch 16/100] [score 240.80]\n",
    "# [Test on environment] [epoch 18/100] [score 230.40]\n",
    "# [epoch   20/100] [iter   30000] [loss 0.00440]\n",
    "# [Test on environment] [epoch 20/100] [score 283.10]\n",
    "# [Test on environment] [epoch 22/100] [score 249.60]\n",
    "# [Test on environment] [epoch 24/100] [score 259.50]\n",
    "# [epoch   26/100] [iter   40000] [loss 0.00377]\n",
    "# [Test on environment] [epoch 26/100] [score 251.80]\n",
    "# [Test on environment] [epoch 28/100] [score 235.70]\n",
    "# [Test on environment] [epoch 30/100] [score 225.20]\n",
    "# [Test on environment] [epoch 32/100] [score 287.80]\n",
    "# [epoch   33/100] [iter   50000] [loss 0.01454]\n",
    "# [Test on environment] [epoch 34/100] [score 257.30]\n",
    "# [Test on environment] [epoch 36/100] [score 253.90]\n",
    "# [Test on environment] [epoch 38/100] [score 242.10]\n",
    "# [epoch   39/100] [iter   60000] [loss 0.00070]\n",
    "# [Test on environment] [epoch 40/100] [score 234.90]\n",
    "# [Test on environment] [epoch 42/100] [score 237.00]\n",
    "# [Test on environment] [epoch 44/100] [score 253.80]\n",
    "# [epoch   45/100] [iter   70000] [loss 0.00645]\n",
    "# [Test on environment] [epoch 46/100] [score 290.80]\n",
    "# [Test on environment] [epoch 48/100] [score 247.60]\n",
    "# [Test on environment] [epoch 50/100] [score 257.00]\n",
    "# [epoch   52/100] [iter   80000] [loss 0.00000]\n",
    "# [Test on environment] [epoch 52/100] [score 262.00]\n",
    "# [Test on environment] [epoch 54/100] [score 276.90]\n",
    "# [Test on environment] [epoch 56/100] [score 250.90]\n",
    "# [epoch   58/100] [iter   90000] [loss 0.00122]\n",
    "# [Test on environment] [epoch 58/100] [score 243.00]\n",
    "# [Test on environment] [epoch 60/100] [score 235.90]\n",
    "# [Test on environment] [epoch 62/100] [score 266.10]\n",
    "# [Test on environment] [epoch 64/100] [score 249.90]\n",
    "# [epoch   65/100] [iter  100000] [loss 0.00000]\n",
    "# [Test on environment] [epoch 66/100] [score 253.70]\n",
    "# [Test on environment] [epoch 68/100] [score 268.20]\n",
    "# [Test on environment] [epoch 70/100] [score 269.80]\n",
    "# [epoch   71/100] [iter  110000] [loss 0.00079]\n",
    "# [Test on environment] [epoch 72/100] [score 276.80]\n",
    "# [Test on environment] [epoch 74/100] [score 232.60]\n",
    "# [Test on environment] [epoch 76/100] [score 258.60]\n",
    "# [epoch   78/100] [iter  120000] [loss 0.00002]\n",
    "# [Test on environment] [epoch 78/100] [score 259.10]\n",
    "# [Test on environment] [epoch 80/100] [score 243.60]\n",
    "# [Test on environment] [epoch 82/100] [score 265.50]\n",
    "# [epoch   84/100] [iter  130000] [loss 0.01828]\n",
    "# [Test on environment] [epoch 84/100] [score 238.30]\n",
    "# [Test on environment] [epoch 86/100] [score 246.30]\n",
    "# [Test on environment] [epoch 88/100] [score 250.30]\n",
    "# [epoch   90/100] [iter  140000] [loss 0.11484]\n",
    "# [Test on environment] [epoch 90/100] [score 268.30]\n",
    "# [Test on environment] [epoch 92/100] [score 248.90]\n",
    "# [Test on environment] [epoch 94/100] [score 226.10]\n",
    "# [Test on environment] [epoch 96/100] [score 274.00]\n",
    "# [epoch   97/100] [iter  150000] [loss 0.00001]\n",
    "# [Test on environment] [epoch 98/100] [score 279.90]\n",
    "# [Test on environment] [epoch 100/100] [score 271.10]\n",
    "# Saving model as behavioral_cloning_CartPole-v1.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 pts]** Did you manage to learn a good policy? How consistent is the reward you are getting?\n",
    "\n",
    "**Answer:** Yes the policy is good and fairly consistent, with reward around 250. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. [24 pts] Deep Q Learning\n",
    "\n",
    "There are two main issues with the behavior cloning approach.\n",
    "\n",
    "- First, we are not always lucky enough to have access to a dataset of expert demonstrations.\n",
    "- Second, replicating an expert policy suffers from compounding error. The policy $\\pi$ only sees these \"perfect\" examples and has no knowledge on how to recover from states not visited by the expert. For this reason, as soon as it is presented with a state that is off the expert trajectory, it will perform poorly and will continue to deviate from a good trajectory without the possibility of recovering from errors.\n",
    "\n",
    "---\n",
    "The second task consists in solving the environment from scratch, using RL, and most specifically the DQN algorithm, to learn a policy $\\pi$.\n",
    "\n",
    "For this task, familiarize yourself with the file `dqn.py`. We are going to re-use the file `model.py` for the model you created in the previous task.\n",
    "\n",
    "Your task is very similar to the one in the previous assignment, to implement the Q-learning algorithm, but in this version, our Q-function is approximated with a neural network.\n",
    "\n",
    "The algorithm (excerpted from [Atari DQN paper](https://arxiv.org/abs/1312.5602)) is given below:\n",
    "\n",
    "![DQN algorithm](https://i.imgur.com/Mh4Uxta.png)\n",
    "\n",
    "### 2.0 [2 pts] Think about your model...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In DQN, we are using the same model as in task 1 for behavioral cloning. In both tasks the model receives as input the state and in both tasks the model outputs something that has the same dimensionality as the number of actions. These two outputs, though, represent very different things. What is each one representing?\n",
    "\n",
    "**Answer:** Behavioral cloning model output: the probability of taking either action; DQN model output: Q value (expected cumulative rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 [10 pts] Update your Q-function\n",
    "\n",
    "Complete the `optimize_model` function. This function receives as input a `state`, an `action`, the `next_state`, the `reward` and `done` representing the tuple $(s_t, a_t, s_{t+1}, r_t, done_t)$. Your task is to update your Q-function as shown in the [Atari DQN paper](https://arxiv.org/abs/1312.5602) environment. For now don't be concerned with the experience replay buffer. We'll get to that later.\n",
    "\n",
    "![Loss function](https://i.imgur.com/tpTsV8m.png)\n",
    "\n",
    "Insert your code in the placeholder below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLACEHOLDER TO INSERT YOUR optimize_model function here:\n",
    "\n",
    "# def optimize_model(state, action, next_state, reward, done):\n",
    "#     # TODO given a tuple (s_t, a_t, s_{t+1}, r_t, done_t) update your model weights\n",
    "\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "###Answer###\n",
    "# def optimize_model(state, action, next_state, reward, done):\n",
    "#     # tensor conversion\n",
    "#     state_tensor = torch.tensor([state], dtype=torch.float32).to(device)\n",
    "#     action_tensor = torch.tensor([[action]], dtype=torch.long).to(device)\n",
    "#     reward_tensor = torch.tensor([reward], dtype=torch.float32).to(device)\n",
    "#     next_state_tensor = torch.tensor([next_state], dtype=torch.float32).to(device)\n",
    "#     done_tensor = torch.tensor([done], dtype=torch.float32).to(device)\n",
    "\n",
    "#     # get q\n",
    "#     q_values = model(state_tensor)\n",
    "#     state_action_value = q_values.gather(1, action_tensor) #current q\n",
    "\n",
    "#     with torch.no_grad(): #target q\n",
    "#         next_q_values = target(next_state_tensor)\n",
    "#         next_state_value = next_q_values.max(1)[0]\n",
    "#         next_state_value = next_state_value * (1 - done_tensor)\n",
    "#         expected_state_action_value = reward_tensor + GAMMA * next_state_value\n",
    "\n",
    "#     loss = F.mse_loss(state_action_value, expected_state_action_value)\n",
    "\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 [5 pts] $\\epsilon$-greedy strategy\n",
    "\n",
    "You will need a strategy to explore your environment. The standard strategy is to use $\\epsilon$-greedy. Implement it in the `choose_action` function template.\n",
    "\n",
    "Insert your code in the placeholder below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLACEHOLDER TO INSERT YOUR choose_action function here:\n",
    "\n",
    "# def choose_action(state, test_mode=False):\n",
    "#     # TODO implement an epsilon-greedy strategy\n",
    "#     raise NotImplementedError()\n",
    "\n",
    "###Answer###\n",
    "# def choose_action(state, test_mode=False):\n",
    "#     epsilon = EPS_EXPLORATION if not test_mode else 0\n",
    "#     state_tensor = torch.tensor([state], dtype=torch.float32).to(device)\n",
    "#     if random.random() < epsilon:\n",
    "#         action = torch.tensor([[random.randrange(n_actions)]], dtype=torch.long).to(device)\n",
    "#     else:\n",
    "#         with torch.no_grad():\n",
    "#             q_values = model(state_tensor)\n",
    "#             action = torch.argmax(q_values, dim=1).unsqueeze(0)\n",
    "#     return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 [2 pts] Train your model\n",
    "\n",
    "Try to train a model in this way.\n",
    "\n",
    "You can run your code by doing:\n",
    "\n",
    "```\n",
    "python3 dqn.py\n",
    "```\n",
    "\n",
    "How many episodes does it take to learn (ie. reach a good reward)?\n",
    "\n",
    "**Answer:** It took ~1650 episodes to reach a good reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## PASTE YOUR TERMINAL OUTPUT HERE\n",
    "# # NOTE: TO HAVE LESS LINES PRINTED, YOU CAN SET THE VARIABLE PRINT_INTERVAL TO 5 or 10\n",
    "# [Episode   10/4000] [Steps   10] [reward 11.0]\n",
    "# [Episode   20/4000] [Steps   12] [reward 13.0]\n",
    "# ----------\n",
    "# saving model.\n",
    "# [TEST Episode 25] [Average Reward 9.5]\n",
    "# ----------\n",
    "# [Episode   30/4000] [Steps   10] [reward 11.0]\n",
    "# [Episode   40/4000] [Steps    8] [reward 9.0]\n",
    "# [Episode   50/4000] [Steps    9] [reward 10.0]\n",
    "# ----------\n",
    "# [TEST Episode 50] [Average Reward 9.4]\n",
    "# ----------\n",
    "# [Episode   60/4000] [Steps   11] [reward 12.0]\n",
    "# [Episode   70/4000] [Steps    9] [reward 10.0]\n",
    "# ----------\n",
    "# [TEST Episode 75] [Average Reward 9.5]\n",
    "# ----------\n",
    "# [Episode   80/4000] [Steps    9] [reward 10.0]\n",
    "# [Episode   90/4000] [Steps    9] [reward 10.0]\n",
    "# [Episode  100/4000] [Steps    8] [reward 9.0]\n",
    "# ----------\n",
    "# [TEST Episode 100] [Average Reward 9.4]\n",
    "# ----------\n",
    "# [Episode  110/4000] [Steps    9] [reward 10.0]\n",
    "# [Episode  120/4000] [Steps   11] [reward 12.0]\n",
    "# ----------\n",
    "# saving model.\n",
    "# [TEST Episode 125] [Average Reward 9.9]\n",
    "# ----------\n",
    "# [Episode  130/4000] [Steps   10] [reward 11.0]\n",
    "# [Episode  140/4000] [Steps   11] [reward 12.0]\n",
    "# [Episode  150/4000] [Steps    9] [reward 10.0]\n",
    "# ----------\n",
    "# saving model.\n",
    "# [TEST Episode 150] [Average Reward 10.9]\n",
    "# ----------\n",
    "# [Episode  160/4000] [Steps    8] [reward 9.0]\n",
    "# [Episode  170/4000] [Steps   13] [reward 14.0]\n",
    "# ----------\n",
    "# saving model.\n",
    "# [TEST Episode 175] [Average Reward 15.0]\n",
    "# ----------\n",
    "# [Episode  180/4000] [Steps   32] [reward 33.0]\n",
    "# [Episode  190/4000] [Steps   21] [reward 22.0]\n",
    "# [Episode  200/4000] [Steps    8] [reward 9.0]\n",
    "# ----------\n",
    "# [TEST Episode 200] [Average Reward 11.1]\n",
    "# ----------\n",
    "# [Episode  210/4000] [Steps   10] [reward 11.0]\n",
    "# [Episode  220/4000] [Steps    8] [reward 9.0]\n",
    "# ----------\n",
    "# [TEST Episode 225] [Average Reward 14.7]\n",
    "# ----------\n",
    "# [Episode  230/4000] [Steps    7] [reward 8.0]\n",
    "# [Episode  240/4000] [Steps    9] [reward 10.0]\n",
    "# [Episode  250/4000] [Steps   10] [reward 11.0]\n",
    "# ----------\n",
    "# [TEST Episode 250] [Average Reward 9.9]\n",
    "# ----------\n",
    "# [Episode  260/4000] [Steps    9] [reward 10.0]\n",
    "# [Episode  270/4000] [Steps    7] [reward 8.0]\n",
    "# ----------\n",
    "# [TEST Episode 275] [Average Reward 10.1]\n",
    "# ----------\n",
    "# [Episode  280/4000] [Steps   10] [reward 11.0]\n",
    "# [Episode  290/4000] [Steps    9] [reward 10.0]\n",
    "# [Episode  300/4000] [Steps   11] [reward 12.0]\n",
    "# ----------\n",
    "# [TEST Episode 300] [Average Reward 10.2]\n",
    "# ----------\n",
    "# [Episode  310/4000] [Steps   10] [reward 11.0]\n",
    "# [Episode  320/4000] [Steps   14] [reward 15.0]\n",
    "# ----------\n",
    "# saving model.\n",
    "# [TEST Episode 325] [Average Reward 15.3]\n",
    "# ----------\n",
    "# [Episode  330/4000] [Steps   42] [reward 43.0]\n",
    "# [Episode  340/4000] [Steps   27] [reward 28.0]\n",
    "# [Episode  350/4000] [Steps   11] [reward 12.0]\n",
    "# ----------\n",
    "# saving model.\n",
    "# [TEST Episode 350] [Average Reward 33.2]\n",
    "# ----------\n",
    "# [Episode  360/4000] [Steps   13] [reward 14.0]\n",
    "# [Episode  370/4000] [Steps   40] [reward 41.0]\n",
    "# ----------\n",
    "# [TEST Episode 375] [Average Reward 15.2]\n",
    "# ----------\n",
    "# [Episode  380/4000] [Steps   11] [reward 12.0]\n",
    "# [Episode  390/4000] [Steps   15] [reward 16.0]\n",
    "# [Episode  400/4000] [Steps   18] [reward 19.0]\n",
    "# ----------\n",
    "# [TEST Episode 400] [Average Reward 16.5]\n",
    "# ----------\n",
    "# [Episode  410/4000] [Steps   16] [reward 17.0]\n",
    "# [Episode  420/4000] [Steps   17] [reward 18.0]\n",
    "# ----------\n",
    "# [TEST Episode 425] [Average Reward 27.8]\n",
    "# ----------\n",
    "# [Episode  430/4000] [Steps   13] [reward 14.0]\n",
    "# [Episode  440/4000] [Steps   20] [reward 21.0]\n",
    "# [Episode  450/4000] [Steps   80] [reward 81.0]\n",
    "# ----------\n",
    "# [TEST Episode 450] [Average Reward 18.4]\n",
    "# ----------\n",
    "# [Episode  460/4000] [Steps   18] [reward 19.0]\n",
    "# [Episode  470/4000] [Steps   11] [reward 12.0]\n",
    "# ----------\n",
    "# [TEST Episode 475] [Average Reward 25.4]\n",
    "# ----------\n",
    "# [Episode  480/4000] [Steps   17] [reward 18.0]\n",
    "# [Episode  490/4000] [Steps   33] [reward 34.0]\n",
    "# [Episode  500/4000] [Steps   19] [reward 20.0]\n",
    "# ----------\n",
    "# [TEST Episode 500] [Average Reward 14.6]\n",
    "# ----------\n",
    "# [Episode  510/4000] [Steps    7] [reward 8.0]\n",
    "# [Episode  520/4000] [Steps    8] [reward 9.0]\n",
    "# ----------\n",
    "# [TEST Episode 525] [Average Reward 9.5]\n",
    "# ----------\n",
    "# [Episode  530/4000] [Steps   11] [reward 12.0]\n",
    "# [Episode  540/4000] [Steps   11] [reward 12.0]\n",
    "# [Episode  550/4000] [Steps    8] [reward 9.0]\n",
    "# ----------\n",
    "# [TEST Episode 550] [Average Reward 10.2]\n",
    "# ----------\n",
    "# [Episode  560/4000] [Steps   12] [reward 13.0]\n",
    "# [Episode  570/4000] [Steps   11] [reward 12.0]\n",
    "# ----------\n",
    "# [TEST Episode 575] [Average Reward 9.3]\n",
    "# ----------\n",
    "# [Episode  580/4000] [Steps   13] [reward 14.0]\n",
    "# [Episode  590/4000] [Steps    8] [reward 9.0]\n",
    "# [Episode  600/4000] [Steps   34] [reward 35.0]\n",
    "# ----------\n",
    "# [TEST Episode 600] [Average Reward 28.1]\n",
    "# ----------\n",
    "# [Episode  610/4000] [Steps    8] [reward 9.0]\n",
    "# [Episode  620/4000] [Steps    8] [reward 9.0]\n",
    "# ----------\n",
    "# [TEST Episode 625] [Average Reward 31.8]\n",
    "# ----------\n",
    "# [Episode  630/4000] [Steps   21] [reward 22.0]\n",
    "# [Episode  640/4000] [Steps   31] [reward 32.0]\n",
    "# [Episode  650/4000] [Steps   24] [reward 25.0]\n",
    "# ----------\n",
    "# [TEST Episode 650] [Average Reward 17.2]\n",
    "# ----------\n",
    "# [Episode  660/4000] [Steps    7] [reward 8.0]\n",
    "# [Episode  670/4000] [Steps   17] [reward 18.0]\n",
    "# ----------\n",
    "# [TEST Episode 675] [Average Reward 9.2]\n",
    "# ----------\n",
    "# [Episode  680/4000] [Steps    8] [reward 9.0]\n",
    "# [Episode  690/4000] [Steps   10] [reward 11.0]\n",
    "# [Episode  700/4000] [Steps   12] [reward 13.0]\n",
    "# ----------\n",
    "# [TEST Episode 700] [Average Reward 9.4]\n",
    "# ----------\n",
    "# [Episode  710/4000] [Steps    9] [reward 10.0]\n",
    "# [Episode  720/4000] [Steps   16] [reward 17.0]\n",
    "# ----------\n",
    "# [TEST Episode 725] [Average Reward 16.5]\n",
    "# ----------\n",
    "# [Episode  730/4000] [Steps   10] [reward 11.0]\n",
    "# [Episode  740/4000] [Steps    9] [reward 10.0]\n",
    "# [Episode  750/4000] [Steps    9] [reward 10.0]\n",
    "# ----------\n",
    "# [TEST Episode 750] [Average Reward 15.0]\n",
    "# ----------\n",
    "# [Episode  760/4000] [Steps   10] [reward 11.0]\n",
    "# [Episode  770/4000] [Steps   16] [reward 17.0]\n",
    "# ----------\n",
    "# [TEST Episode 775] [Average Reward 9.3]\n",
    "# ----------\n",
    "# [Episode  780/4000] [Steps    9] [reward 10.0]\n",
    "# [Episode  790/4000] [Steps    9] [reward 10.0]\n",
    "# [Episode  800/4000] [Steps   11] [reward 12.0]\n",
    "# ----------\n",
    "# [TEST Episode 800] [Average Reward 8.8]\n",
    "# ----------\n",
    "# [Episode  810/4000] [Steps   20] [reward 21.0]\n",
    "# [Episode  820/4000] [Steps   12] [reward 13.0]\n",
    "# ----------\n",
    "# [TEST Episode 825] [Average Reward 25.1]\n",
    "# ----------\n",
    "# [Episode  830/4000] [Steps   29] [reward 30.0]\n",
    "# [Episode  840/4000] [Steps   11] [reward 12.0]\n",
    "# [Episode  850/4000] [Steps   11] [reward 12.0]\n",
    "# ----------\n",
    "# [TEST Episode 850] [Average Reward 10.0]\n",
    "# ----------\n",
    "# [Episode  860/4000] [Steps   18] [reward 19.0]\n",
    "# [Episode  870/4000] [Steps   20] [reward 21.0]\n",
    "# ----------\n",
    "# [TEST Episode 875] [Average Reward 14.4]\n",
    "# ----------\n",
    "# [Episode  880/4000] [Steps   83] [reward 84.0]\n",
    "# [Episode  890/4000] [Steps    8] [reward 9.0]\n",
    "# [Episode  900/4000] [Steps    8] [reward 9.0]\n",
    "# ----------\n",
    "# [TEST Episode 900] [Average Reward 9.9]\n",
    "# ----------\n",
    "# [Episode  910/4000] [Steps   11] [reward 12.0]\n",
    "# [Episode  920/4000] [Steps   85] [reward 86.0]\n",
    "# ----------\n",
    "# [TEST Episode 925] [Average Reward 15.2]\n",
    "# ----------\n",
    "# [Episode  930/4000] [Steps   18] [reward 19.0]\n",
    "# [Episode  940/4000] [Steps   10] [reward 11.0]\n",
    "# [Episode  950/4000] [Steps   62] [reward 63.0]\n",
    "# ----------\n",
    "# [TEST Episode 950] [Average Reward 19.7]\n",
    "# ----------\n",
    "# [Episode  960/4000] [Steps   14] [reward 15.0]\n",
    "# [Episode  970/4000] [Steps   79] [reward 80.0]\n",
    "# ----------\n",
    "# [TEST Episode 975] [Average Reward 14.4]\n",
    "# ----------\n",
    "# [Episode  980/4000] [Steps   15] [reward 16.0]\n",
    "# [Episode  990/4000] [Steps   10] [reward 11.0]\n",
    "# [Episode 1000/4000] [Steps    8] [reward 9.0]\n",
    "# ----------\n",
    "# [TEST Episode 1000] [Average Reward 9.3]\n",
    "# ----------\n",
    "# [Episode 1010/4000] [Steps   16] [reward 17.0]\n",
    "# [Episode 1020/4000] [Steps   18] [reward 19.0]\n",
    "# ----------\n",
    "# saving model.\n",
    "# [TEST Episode 1025] [Average Reward 41.6]\n",
    "# ----------\n",
    "# [Episode 1030/4000] [Steps  111] [reward 112.0]\n",
    "# [Episode 1040/4000] [Steps   16] [reward 17.0]\n",
    "# [Episode 1050/4000] [Steps   29] [reward 30.0]\n",
    "# ----------\n",
    "# [TEST Episode 1050] [Average Reward 20.9]\n",
    "# ----------\n",
    "# [Episode 1060/4000] [Steps    8] [reward 9.0]\n",
    "# [Episode 1070/4000] [Steps   10] [reward 11.0]\n",
    "# ----------\n",
    "# [TEST Episode 1075] [Average Reward 9.5]\n",
    "# ----------\n",
    "# [Episode 1080/4000] [Steps    9] [reward 10.0]\n",
    "# [Episode 1090/4000] [Steps   15] [reward 16.0]\n",
    "# [Episode 1100/4000] [Steps   60] [reward 61.0]\n",
    "# ----------\n",
    "# [TEST Episode 1100] [Average Reward 17.2]\n",
    "# ----------\n",
    "# [Episode 1110/4000] [Steps   18] [reward 19.0]\n",
    "# [Episode 1120/4000] [Steps    8] [reward 9.0]\n",
    "# ----------\n",
    "# [TEST Episode 1125] [Average Reward 17.9]\n",
    "# ----------\n",
    "# [Episode 1130/4000] [Steps   15] [reward 16.0]\n",
    "# [Episode 1140/4000] [Steps   18] [reward 19.0]\n",
    "# [Episode 1150/4000] [Steps   21] [reward 22.0]\n",
    "# ----------\n",
    "# [TEST Episode 1150] [Average Reward 15.7]\n",
    "# ----------\n",
    "# [Episode 1160/4000] [Steps   36] [reward 37.0]\n",
    "# [Episode 1170/4000] [Steps  499] [reward 500.0]\n",
    "# ----------\n",
    "# saving model.\n",
    "# [TEST Episode 1175] [Average Reward 75.9]\n",
    "# ----------\n",
    "# [Episode 1180/4000] [Steps   12] [reward 13.0]\n",
    "# [Episode 1190/4000] [Steps    8] [reward 9.0]\n",
    "# [Episode 1200/4000] [Steps    8] [reward 9.0]\n",
    "# ----------\n",
    "# [TEST Episode 1200] [Average Reward 9.7]\n",
    "# ----------\n",
    "# [Episode 1210/4000] [Steps   14] [reward 15.0]\n",
    "# [Episode 1220/4000] [Steps   14] [reward 15.0]\n",
    "# ----------\n",
    "# [TEST Episode 1225] [Average Reward 19.0]\n",
    "# ----------\n",
    "# [Episode 1230/4000] [Steps   21] [reward 22.0]\n",
    "# [Episode 1240/4000] [Steps   88] [reward 89.0]\n",
    "# [Episode 1250/4000] [Steps   11] [reward 12.0]\n",
    "# ----------\n",
    "# [TEST Episode 1250] [Average Reward 15.4]\n",
    "# ----------\n",
    "# [Episode 1260/4000] [Steps   10] [reward 11.0]\n",
    "# [Episode 1270/4000] [Steps   16] [reward 17.0]\n",
    "# ----------\n",
    "# [TEST Episode 1275] [Average Reward 14.1]\n",
    "# ----------\n",
    "# [Episode 1280/4000] [Steps   12] [reward 13.0]\n",
    "# [Episode 1290/4000] [Steps   15] [reward 16.0]\n",
    "# [Episode 1300/4000] [Steps   18] [reward 19.0]\n",
    "# ----------\n",
    "# [TEST Episode 1300] [Average Reward 14.8]\n",
    "# ----------\n",
    "# [Episode 1310/4000] [Steps   15] [reward 16.0]\n",
    "# [Episode 1320/4000] [Steps   22] [reward 23.0]\n",
    "# ----------\n",
    "# [TEST Episode 1325] [Average Reward 16.2]\n",
    "# ----------\n",
    "# [Episode 1330/4000] [Steps   95] [reward 96.0]\n",
    "# [Episode 1340/4000] [Steps   12] [reward 13.0]\n",
    "# [Episode 1350/4000] [Steps  190] [reward 191.0]\n",
    "# ----------\n",
    "# [TEST Episode 1350] [Average Reward 15.7]\n",
    "# ----------\n",
    "# [Episode 1360/4000] [Steps   21] [reward 22.0]\n",
    "# [Episode 1370/4000] [Steps   12] [reward 13.0]\n",
    "# ----------\n",
    "# [TEST Episode 1375] [Average Reward 9.9]\n",
    "# ----------\n",
    "# [Episode 1380/4000] [Steps   17] [reward 18.0]\n",
    "# [Episode 1390/4000] [Steps   65] [reward 66.0]\n",
    "# [Episode 1400/4000] [Steps   12] [reward 13.0]\n",
    "# ----------\n",
    "# [TEST Episode 1400] [Average Reward 15.3]\n",
    "# ----------\n",
    "# [Episode 1410/4000] [Steps   15] [reward 16.0]\n",
    "# [Episode 1420/4000] [Steps   15] [reward 16.0]\n",
    "# ----------\n",
    "# [TEST Episode 1425] [Average Reward 15.4]\n",
    "# ----------\n",
    "# [Episode 1430/4000] [Steps  160] [reward 161.0]\n",
    "# [Episode 1440/4000] [Steps   12] [reward 13.0]\n",
    "# [Episode 1450/4000] [Steps   18] [reward 19.0]\n",
    "# ----------\n",
    "# saving model.\n",
    "# [TEST Episode 1450] [Average Reward 76.9]\n",
    "# ----------\n",
    "# [Episode 1460/4000] [Steps   40] [reward 41.0]\n",
    "# [Episode 1470/4000] [Steps   60] [reward 61.0]\n",
    "# ----------\n",
    "# [TEST Episode 1475] [Average Reward 70.7]\n",
    "# ----------\n",
    "# [Episode 1480/4000] [Steps    9] [reward 10.0]\n",
    "# [Episode 1490/4000] [Steps   65] [reward 66.0]\n",
    "# [Episode 1500/4000] [Steps  111] [reward 112.0]\n",
    "# ----------\n",
    "# [TEST Episode 1500] [Average Reward 17.7]\n",
    "# ----------\n",
    "# [Episode 1510/4000] [Steps   79] [reward 80.0]\n",
    "# [Episode 1520/4000] [Steps   11] [reward 12.0]\n",
    "# ----------\n",
    "# [TEST Episode 1525] [Average Reward 51.9]\n",
    "# ----------\n",
    "# [Episode 1530/4000] [Steps   14] [reward 15.0]\n",
    "# [Episode 1540/4000] [Steps    8] [reward 9.0]\n",
    "# [Episode 1550/4000] [Steps   15] [reward 16.0]\n",
    "# ----------\n",
    "# [TEST Episode 1550] [Average Reward 28.1]\n",
    "# ----------\n",
    "# [Episode 1560/4000] [Steps   11] [reward 12.0]\n",
    "# [Episode 1570/4000] [Steps   13] [reward 14.0]\n",
    "# ----------\n",
    "# [TEST Episode 1575] [Average Reward 15.7]\n",
    "# ----------\n",
    "# [Episode 1580/4000] [Steps   13] [reward 14.0]\n",
    "# [Episode 1590/4000] [Steps   63] [reward 64.0]\n",
    "# [Episode 1600/4000] [Steps   16] [reward 17.0]\n",
    "# ----------\n",
    "# [TEST Episode 1600] [Average Reward 66.4]\n",
    "# ----------\n",
    "# [Episode 1610/4000] [Steps   44] [reward 45.0]\n",
    "# [Episode 1620/4000] [Steps   11] [reward 12.0]\n",
    "# ----------\n",
    "# saving model.\n",
    "# [TEST Episode 1625] [Average Reward 94.7]\n",
    "# ----------\n",
    "# [Episode 1630/4000] [Steps  108] [reward 109.0]\n",
    "# [Episode 1640/4000] [Steps   15] [reward 16.0]\n",
    "# [Episode 1650/4000] [Steps   11] [reward 12.0]\n",
    "# ----------\n",
    "# saving model.\n",
    "# [TEST Episode 1650] [Average Reward 151.4]\n",
    "# ----------\n",
    "# [Episode 1660/4000] [Steps   15] [reward 16.0]\n",
    "# [Episode 1670/4000] [Steps   25] [reward 26.0]\n",
    "# ----------\n",
    "# [TEST Episode 1675] [Average Reward 127.8]\n",
    "# ----------\n",
    "# [Episode 1680/4000] [Steps  278] [reward 279.0]\n",
    "# [Episode 1690/4000] [Steps  167] [reward 168.0]\n",
    "# [Episode 1700/4000] [Steps   86] [reward 87.0]\n",
    "# ----------\n",
    "# [TEST Episode 1700] [Average Reward 43.8]\n",
    "# ----------\n",
    "# [Episode 1710/4000] [Steps    9] [reward 10.0]\n",
    "# [Episode 1720/4000] [Steps   25] [reward 26.0]\n",
    "# ----------\n",
    "# [TEST Episode 1725] [Average Reward 14.5]\n",
    "# ----------\n",
    "# [Episode 1730/4000] [Steps   14] [reward 15.0]\n",
    "# [Episode 1740/4000] [Steps   15] [reward 16.0]\n",
    "# [Episode 1750/4000] [Steps  151] [reward 152.0]\n",
    "# ----------\n",
    "# [TEST Episode 1750] [Average Reward 33.8]\n",
    "# ----------\n",
    "# [Episode 1760/4000] [Steps   59] [reward 60.0]\n",
    "# [Episode 1770/4000] [Steps   10] [reward 11.0]\n",
    "# ----------\n",
    "# [TEST Episode 1775] [Average Reward 99.4]\n",
    "# ----------\n",
    "# [Episode 1780/4000] [Steps  321] [reward 322.0]\n",
    "# [Episode 1790/4000] [Steps   39] [reward 40.0]\n",
    "# [Episode 1800/4000] [Steps  147] [reward 148.0]\n",
    "# ----------\n",
    "# [TEST Episode 1800] [Average Reward 120.1]\n",
    "# ----------\n",
    "# [Episode 1810/4000] [Steps   95] [reward 96.0]\n",
    "# [Episode 1820/4000] [Steps  123] [reward 124.0]\n",
    "# ----------\n",
    "# [TEST Episode 1825] [Average Reward 83.4]\n",
    "# ----------\n",
    "# [Episode 1830/4000] [Steps  108] [reward 109.0]\n",
    "# [Episode 1840/4000] [Steps    7] [reward 8.0]\n",
    "# [Episode 1850/4000] [Steps   12] [reward 13.0]\n",
    "# ----------\n",
    "# [TEST Episode 1850] [Average Reward 11.7]\n",
    "# ----------\n",
    "# [Episode 1860/4000] [Steps  128] [reward 129.0]\n",
    "# [Episode 1870/4000] [Steps  323] [reward 324.0]\n",
    "# ----------\n",
    "# [TEST Episode 1875] [Average Reward 11.5]\n",
    "# ----------\n",
    "# [Episode 1880/4000] [Steps   16] [reward 17.0]\n",
    "# [Episode 1890/4000] [Steps  106] [reward 107.0]\n",
    "# [Episode 1900/4000] [Steps   10] [reward 11.0]\n",
    "# ----------\n",
    "# [TEST Episode 1900] [Average Reward 9.4]\n",
    "# ----------\n",
    "# [Episode 1910/4000] [Steps   42] [reward 43.0]\n",
    "# [Episode 1920/4000] [Steps   31] [reward 32.0]\n",
    "# ----------\n",
    "# [TEST Episode 1925] [Average Reward 101.2]\n",
    "# ----------\n",
    "# [Episode 1930/4000] [Steps    9] [reward 10.0]\n",
    "# [Episode 1940/4000] [Steps  218] [reward 219.0]\n",
    "# [Episode 1950/4000] [Steps  131] [reward 132.0]\n",
    "# ----------\n",
    "# [TEST Episode 1950] [Average Reward 91.4]\n",
    "# ----------\n",
    "# [Episode 1960/4000] [Steps   13] [reward 14.0]\n",
    "# [Episode 1970/4000] [Steps   12] [reward 13.0]\n",
    "# ----------\n",
    "# [TEST Episode 1975] [Average Reward 78.5]\n",
    "# ----------\n",
    "# [Episode 1980/4000] [Steps  127] [reward 128.0]\n",
    "# [Episode 1990/4000] [Steps   61] [reward 62.0]\n",
    "# [Episode 2000/4000] [Steps   33] [reward 34.0]\n",
    "# ----------\n",
    "# saving model.\n",
    "# [TEST Episode 2000] [Average Reward 448.8]\n",
    "# ----------\n",
    "# [Episode 2010/4000] [Steps   52] [reward 53.0]\n",
    "# [Episode 2020/4000] [Steps  168] [reward 169.0]\n",
    "# ----------\n",
    "# [TEST Episode 2025] [Average Reward 130.8]\n",
    "# ----------\n",
    "# [Episode 2030/4000] [Steps  140] [reward 141.0]\n",
    "# [Episode 2040/4000] [Steps   36] [reward 37.0]\n",
    "# [Episode 2050/4000] [Steps  220] [reward 221.0]\n",
    "# ----------\n",
    "# [TEST Episode 2050] [Average Reward 135.2]\n",
    "# ----------\n",
    "# [Episode 2060/4000] [Steps  191] [reward 192.0]\n",
    "# [Episode 2070/4000] [Steps  241] [reward 242.0]\n",
    "# ----------\n",
    "# [TEST Episode 2075] [Average Reward 239.2]\n",
    "# ----------\n",
    "# [Episode 2080/4000] [Steps  373] [reward 374.0]\n",
    "# [Episode 2090/4000] [Steps   13] [reward 14.0]\n",
    "# [Episode 2100/4000] [Steps  139] [reward 140.0]\n",
    "# ----------\n",
    "# [TEST Episode 2100] [Average Reward 147.0]\n",
    "# ----------\n",
    "# [Episode 2110/4000] [Steps  499] [reward 500.0]\n",
    "# [Episode 2120/4000] [Steps   42] [reward 43.0]\n",
    "# ----------\n",
    "# [TEST Episode 2125] [Average Reward 147.1]\n",
    "# ----------\n",
    "# [Episode 2130/4000] [Steps  184] [reward 185.0]\n",
    "# [Episode 2140/4000] [Steps  170] [reward 171.0]\n",
    "# [Episode 2150/4000] [Steps  106] [reward 107.0]\n",
    "# ----------\n",
    "# [TEST Episode 2150] [Average Reward 226.2]\n",
    "# ----------\n",
    "# [Episode 2160/4000] [Steps  133] [reward 134.0]\n",
    "# [Episode 2170/4000] [Steps  144] [reward 145.0]\n",
    "# ----------\n",
    "# [TEST Episode 2175] [Average Reward 9.0]\n",
    "# ----------\n",
    "# [Episode 2180/4000] [Steps    9] [reward 10.0]\n",
    "# [Episode 2190/4000] [Steps    8] [reward 9.0]\n",
    "# [Episode 2200/4000] [Steps   15] [reward 16.0]\n",
    "# ----------\n",
    "# [TEST Episode 2200] [Average Reward 205.6]\n",
    "# ----------\n",
    "# [Episode 2210/4000] [Steps   18] [reward 19.0]\n",
    "# [Episode 2220/4000] [Steps   10] [reward 11.0]\n",
    "# ----------\n",
    "# [TEST Episode 2225] [Average Reward 141.1]\n",
    "# ----------\n",
    "# [Episode 2230/4000] [Steps  208] [reward 209.0]\n",
    "# [Episode 2240/4000] [Steps  194] [reward 195.0]\n",
    "# [Episode 2250/4000] [Steps  251] [reward 252.0]\n",
    "# ----------\n",
    "# [TEST Episode 2250] [Average Reward 147.1]\n",
    "# ----------\n",
    "# [Episode 2260/4000] [Steps  152] [reward 153.0]\n",
    "# [Episode 2270/4000] [Steps    9] [reward 10.0]\n",
    "# ----------\n",
    "# [TEST Episode 2275] [Average Reward 118.5]\n",
    "# ----------\n",
    "# [Episode 2280/4000] [Steps  134] [reward 135.0]\n",
    "# [Episode 2290/4000] [Steps  140] [reward 141.0]\n",
    "# [Episode 2300/4000] [Steps   12] [reward 13.0]\n",
    "# ----------\n",
    "# [TEST Episode 2300] [Average Reward 183.5]\n",
    "# ----------\n",
    "# [Episode 2310/4000] [Steps  190] [reward 191.0]\n",
    "# [Episode 2320/4000] [Steps   56] [reward 57.0]\n",
    "# ----------\n",
    "# [TEST Episode 2325] [Average Reward 130.3]\n",
    "# ----------\n",
    "# [Episode 2330/4000] [Steps   41] [reward 42.0]\n",
    "# [Episode 2340/4000] [Steps   74] [reward 75.0]\n",
    "# [Episode 2350/4000] [Steps  162] [reward 163.0]\n",
    "# ----------\n",
    "# [TEST Episode 2350] [Average Reward 177.2]\n",
    "# ----------\n",
    "# [Episode 2360/4000] [Steps   32] [reward 33.0]\n",
    "# [Episode 2370/4000] [Steps   11] [reward 12.0]\n",
    "# ----------\n",
    "# [TEST Episode 2375] [Average Reward 200.0]\n",
    "# ----------\n",
    "# [Episode 2380/4000] [Steps   12] [reward 13.0]\n",
    "# [Episode 2390/4000] [Steps   16] [reward 17.0]\n",
    "# [Episode 2400/4000] [Steps   85] [reward 86.0]\n",
    "# ----------\n",
    "# [TEST Episode 2400] [Average Reward 86.3]\n",
    "# ----------\n",
    "# [Episode 2410/4000] [Steps   97] [reward 98.0]\n",
    "# [Episode 2420/4000] [Steps   87] [reward 88.0]\n",
    "# ----------\n",
    "# [TEST Episode 2425] [Average Reward 270.1]\n",
    "# ----------\n",
    "# [Episode 2430/4000] [Steps   54] [reward 55.0]\n",
    "# [Episode 2440/4000] [Steps  171] [reward 172.0]\n",
    "# [Episode 2450/4000] [Steps  179] [reward 180.0]\n",
    "# ----------\n",
    "# [TEST Episode 2450] [Average Reward 138.6]\n",
    "# ----------\n",
    "# [Episode 2460/4000] [Steps  143] [reward 144.0]\n",
    "# [Episode 2470/4000] [Steps   34] [reward 35.0]\n",
    "# ----------\n",
    "# [TEST Episode 2475] [Average Reward 272.9]\n",
    "# ----------\n",
    "# [Episode 2480/4000] [Steps   69] [reward 70.0]\n",
    "# [Episode 2490/4000] [Steps  446] [reward 447.0]\n",
    "# [Episode 2500/4000] [Steps   54] [reward 55.0]\n",
    "# ----------\n",
    "# [TEST Episode 2500] [Average Reward 229.6]\n",
    "# ----------\n",
    "# [Episode 2510/4000] [Steps  118] [reward 119.0]\n",
    "# [Episode 2520/4000] [Steps   92] [reward 93.0]\n",
    "# ----------\n",
    "# [TEST Episode 2525] [Average Reward 232.4]\n",
    "# ----------\n",
    "# [Episode 2530/4000] [Steps   13] [reward 14.0]\n",
    "# [Episode 2540/4000] [Steps    9] [reward 10.0]\n",
    "# [Episode 2550/4000] [Steps  141] [reward 142.0]\n",
    "# ----------\n",
    "# [TEST Episode 2550] [Average Reward 224.0]\n",
    "# ----------\n",
    "# [Episode 2560/4000] [Steps   52] [reward 53.0]\n",
    "# [Episode 2570/4000] [Steps  118] [reward 119.0]\n",
    "# ----------\n",
    "# [TEST Episode 2575] [Average Reward 112.3]\n",
    "# ----------\n",
    "# [Episode 2580/4000] [Steps  155] [reward 156.0]\n",
    "# [Episode 2590/4000] [Steps    9] [reward 10.0]\n",
    "# [Episode 2600/4000] [Steps  238] [reward 239.0]\n",
    "# ----------\n",
    "# [TEST Episode 2600] [Average Reward 241.0]\n",
    "# ----------\n",
    "# [Episode 2610/4000] [Steps  223] [reward 224.0]\n",
    "# [Episode 2620/4000] [Steps  297] [reward 298.0]\n",
    "# ----------\n",
    "# saving model.\n",
    "# [TEST Episode 2625] [Average Reward 500.0]\n",
    "# ----------\n",
    "# [Episode 2630/4000] [Steps  499] [reward 500.0]\n",
    "# [Episode 2640/4000] [Steps  129] [reward 130.0]\n",
    "# [Episode 2650/4000] [Steps   91] [reward 92.0]\n",
    "# ----------\n",
    "# [TEST Episode 2650] [Average Reward 99.1]\n",
    "# ----------\n",
    "# [Episode 2660/4000] [Steps  176] [reward 177.0]\n",
    "# [Episode 2670/4000] [Steps   14] [reward 15.0]\n",
    "# ----------\n",
    "# [TEST Episode 2675] [Average Reward 380.2]\n",
    "# ----------\n",
    "# [Episode 2680/4000] [Steps  377] [reward 378.0]\n",
    "# [Episode 2690/4000] [Steps  366] [reward 367.0]\n",
    "# [Episode 2700/4000] [Steps  293] [reward 294.0]\n",
    "# ----------\n",
    "# [TEST Episode 2700] [Average Reward 213.1]\n",
    "# ----------\n",
    "# [Episode 2710/4000] [Steps  385] [reward 386.0]\n",
    "# [Episode 2720/4000] [Steps  242] [reward 243.0]\n",
    "# ----------\n",
    "# [TEST Episode 2725] [Average Reward 205.9]\n",
    "# ----------\n",
    "# [Episode 2730/4000] [Steps  205] [reward 206.0]\n",
    "# [Episode 2740/4000] [Steps    9] [reward 10.0]\n",
    "# [Episode 2750/4000] [Steps  202] [reward 203.0]\n",
    "# ----------\n",
    "# [TEST Episode 2750] [Average Reward 496.1]\n",
    "# ----------\n",
    "# [Episode 2760/4000] [Steps  192] [reward 193.0]\n",
    "# [Episode 2770/4000] [Steps  187] [reward 188.0]\n",
    "# ----------\n",
    "# [TEST Episode 2775] [Average Reward 500.0]\n",
    "# ----------\n",
    "# [Episode 2780/4000] [Steps   69] [reward 70.0]\n",
    "# [Episode 2790/4000] [Steps   10] [reward 11.0]\n",
    "# [Episode 2800/4000] [Steps  202] [reward 203.0]\n",
    "# ----------\n",
    "# [TEST Episode 2800] [Average Reward 264.8]\n",
    "# ----------\n",
    "# [Episode 2810/4000] [Steps  499] [reward 500.0]\n",
    "# [Episode 2820/4000] [Steps  182] [reward 183.0]\n",
    "# ----------\n",
    "# [TEST Episode 2825] [Average Reward 139.9]\n",
    "# ----------\n",
    "# [Episode 2830/4000] [Steps  109] [reward 110.0]\n",
    "# [Episode 2840/4000] [Steps   50] [reward 51.0]\n",
    "# [Episode 2850/4000] [Steps   33] [reward 34.0]\n",
    "# ----------\n",
    "# [TEST Episode 2850] [Average Reward 164.2]\n",
    "# ----------\n",
    "# [Episode 2860/4000] [Steps   24] [reward 25.0]\n",
    "# [Episode 2870/4000] [Steps  350] [reward 351.0]\n",
    "# ----------\n",
    "# [TEST Episode 2875] [Average Reward 144.1]\n",
    "# ----------\n",
    "# [Episode 2880/4000] [Steps   85] [reward 86.0]\n",
    "# [Episode 2890/4000] [Steps  129] [reward 130.0]\n",
    "# [Episode 2900/4000] [Steps   61] [reward 62.0]\n",
    "# ----------\n",
    "# [TEST Episode 2900] [Average Reward 106.4]\n",
    "# ----------\n",
    "# [Episode 2910/4000] [Steps  128] [reward 129.0]\n",
    "# [Episode 2920/4000] [Steps  397] [reward 398.0]\n",
    "# ----------\n",
    "# [TEST Episode 2925] [Average Reward 111.0]\n",
    "# ----------\n",
    "# [Episode 2930/4000] [Steps   14] [reward 15.0]\n",
    "# [Episode 2940/4000] [Steps  116] [reward 117.0]\n",
    "# [Episode 2950/4000] [Steps   72] [reward 73.0]\n",
    "# ----------\n",
    "# [TEST Episode 2950] [Average Reward 132.8]\n",
    "# ----------\n",
    "# [Episode 2960/4000] [Steps  106] [reward 107.0]\n",
    "# [Episode 2970/4000] [Steps  132] [reward 133.0]\n",
    "# ----------\n",
    "# [TEST Episode 2975] [Average Reward 107.8]\n",
    "# ----------\n",
    "# [Episode 2980/4000] [Steps   91] [reward 92.0]\n",
    "# [Episode 2990/4000] [Steps  126] [reward 127.0]\n",
    "# [Episode 3000/4000] [Steps  402] [reward 403.0]\n",
    "# ----------\n",
    "# [TEST Episode 3000] [Average Reward 101.2]\n",
    "# ----------\n",
    "# [Episode 3010/4000] [Steps  106] [reward 107.0]\n",
    "# [Episode 3020/4000] [Steps   16] [reward 17.0]\n",
    "# ----------\n",
    "# [TEST Episode 3025] [Average Reward 79.1]\n",
    "# ----------\n",
    "# [Episode 3030/4000] [Steps   96] [reward 97.0]\n",
    "# [Episode 3040/4000] [Steps    9] [reward 10.0]\n",
    "# [Episode 3050/4000] [Steps    8] [reward 9.0]\n",
    "# ----------\n",
    "# [TEST Episode 3050] [Average Reward 11.4]\n",
    "# ----------\n",
    "# [Episode 3060/4000] [Steps  112] [reward 113.0]\n",
    "# [Episode 3070/4000] [Steps   73] [reward 74.0]\n",
    "# ----------\n",
    "# [TEST Episode 3075] [Average Reward 114.7]\n",
    "# ----------\n",
    "# [Episode 3080/4000] [Steps  166] [reward 167.0]\n",
    "# [Episode 3090/4000] [Steps    9] [reward 10.0]\n",
    "# [Episode 3100/4000] [Steps   31] [reward 32.0]\n",
    "# ----------\n",
    "# [TEST Episode 3100] [Average Reward 75.8]\n",
    "# ----------\n",
    "# [Episode 3110/4000] [Steps  121] [reward 122.0]\n",
    "# [Episode 3120/4000] [Steps  139] [reward 140.0]\n",
    "# ----------\n",
    "# [TEST Episode 3125] [Average Reward 101.7]\n",
    "# ----------\n",
    "# [Episode 3130/4000] [Steps  239] [reward 240.0]\n",
    "# [Episode 3140/4000] [Steps  179] [reward 180.0]\n",
    "# [Episode 3150/4000] [Steps   34] [reward 35.0]\n",
    "# ----------\n",
    "# [TEST Episode 3150] [Average Reward 74.3]\n",
    "# ----------\n",
    "# [Episode 3160/4000] [Steps   66] [reward 67.0]\n",
    "# [Episode 3170/4000] [Steps   53] [reward 54.0]\n",
    "# ----------\n",
    "# [TEST Episode 3175] [Average Reward 71.9]\n",
    "# ----------\n",
    "# [Episode 3180/4000] [Steps   89] [reward 90.0]\n",
    "# [Episode 3190/4000] [Steps   48] [reward 49.0]\n",
    "# [Episode 3200/4000] [Steps   53] [reward 54.0]\n",
    "# ----------\n",
    "# [TEST Episode 3200] [Average Reward 71.0]\n",
    "# ----------\n",
    "# [Episode 3210/4000] [Steps    9] [reward 10.0]\n",
    "# [Episode 3220/4000] [Steps   99] [reward 100.0]\n",
    "# ----------\n",
    "# [TEST Episode 3225] [Average Reward 84.6]\n",
    "# ----------\n",
    "# [Episode 3230/4000] [Steps  114] [reward 115.0]\n",
    "# [Episode 3240/4000] [Steps  226] [reward 227.0]\n",
    "# [Episode 3250/4000] [Steps   58] [reward 59.0]\n",
    "# ----------\n",
    "# [TEST Episode 3250] [Average Reward 66.9]\n",
    "# ----------\n",
    "# [Episode 3260/4000] [Steps   45] [reward 46.0]\n",
    "# [Episode 3270/4000] [Steps   80] [reward 81.0]\n",
    "# ----------\n",
    "# [TEST Episode 3275] [Average Reward 363.4]\n",
    "# ----------\n",
    "# [Episode 3280/4000] [Steps   90] [reward 91.0]\n",
    "# [Episode 3290/4000] [Steps   90] [reward 91.0]\n",
    "# [Episode 3300/4000] [Steps   82] [reward 83.0]\n",
    "# ----------\n",
    "# [TEST Episode 3300] [Average Reward 78.8]\n",
    "# ----------\n",
    "# [Episode 3310/4000] [Steps   72] [reward 73.0]\n",
    "# [Episode 3320/4000] [Steps   15] [reward 16.0]\n",
    "# ----------\n",
    "# [TEST Episode 3325] [Average Reward 81.9]\n",
    "# ----------\n",
    "# [Episode 3330/4000] [Steps  109] [reward 110.0]\n",
    "# [Episode 3340/4000] [Steps   55] [reward 56.0]\n",
    "# [Episode 3350/4000] [Steps   11] [reward 12.0]\n",
    "# ----------\n",
    "# [TEST Episode 3350] [Average Reward 500.0]\n",
    "# ----------\n",
    "# [Episode 3360/4000] [Steps  119] [reward 120.0]\n",
    "# [Episode 3370/4000] [Steps   93] [reward 94.0]\n",
    "# ----------\n",
    "# [TEST Episode 3375] [Average Reward 61.1]\n",
    "# ----------\n",
    "# [Episode 3380/4000] [Steps   78] [reward 79.0]\n",
    "# [Episode 3390/4000] [Steps   83] [reward 84.0]\n",
    "# [Episode 3400/4000] [Steps   25] [reward 26.0]\n",
    "# ----------\n",
    "# [TEST Episode 3400] [Average Reward 156.8]\n",
    "# ----------\n",
    "# [Episode 3410/4000] [Steps   39] [reward 40.0]\n",
    "# [Episode 3420/4000] [Steps   14] [reward 15.0]\n",
    "# ----------\n",
    "# [TEST Episode 3425] [Average Reward 85.9]\n",
    "# ----------\n",
    "# [Episode 3430/4000] [Steps   31] [reward 32.0]\n",
    "# [Episode 3440/4000] [Steps   41] [reward 42.0]\n",
    "# [Episode 3450/4000] [Steps   69] [reward 70.0]\n",
    "# ----------\n",
    "# [TEST Episode 3450] [Average Reward 192.1]\n",
    "# ----------\n",
    "# [Episode 3460/4000] [Steps   26] [reward 27.0]\n",
    "# [Episode 3470/4000] [Steps   43] [reward 44.0]\n",
    "# ----------\n",
    "# [TEST Episode 3475] [Average Reward 86.8]\n",
    "# ----------\n",
    "# [Episode 3480/4000] [Steps   13] [reward 14.0]\n",
    "# [Episode 3490/4000] [Steps   40] [reward 41.0]\n",
    "# [Episode 3500/4000] [Steps   53] [reward 54.0]\n",
    "# ----------\n",
    "# [TEST Episode 3500] [Average Reward 498.9]\n",
    "# ----------\n",
    "# [Episode 3510/4000] [Steps    9] [reward 10.0]\n",
    "# [Episode 3520/4000] [Steps   62] [reward 63.0]\n",
    "# ----------\n",
    "# [TEST Episode 3525] [Average Reward 113.0]\n",
    "# ----------\n",
    "# [Episode 3530/4000] [Steps   27] [reward 28.0]\n",
    "# [Episode 3540/4000] [Steps   67] [reward 68.0]\n",
    "# [Episode 3550/4000] [Steps  131] [reward 132.0]\n",
    "# ----------\n",
    "# [TEST Episode 3550] [Average Reward 52.0]\n",
    "# ----------\n",
    "# [Episode 3560/4000] [Steps   17] [reward 18.0]\n",
    "# [Episode 3570/4000] [Steps   25] [reward 26.0]\n",
    "# ----------\n",
    "# [TEST Episode 3575] [Average Reward 362.9]\n",
    "# ----------\n",
    "# [Episode 3580/4000] [Steps   24] [reward 25.0]\n",
    "# [Episode 3590/4000] [Steps   32] [reward 33.0]\n",
    "# [Episode 3600/4000] [Steps   53] [reward 54.0]\n",
    "# ----------\n",
    "# [TEST Episode 3600] [Average Reward 76.8]\n",
    "# ----------\n",
    "# [Episode 3610/4000] [Steps   15] [reward 16.0]\n",
    "# [Episode 3620/4000] [Steps   23] [reward 24.0]\n",
    "# ----------\n",
    "# [TEST Episode 3625] [Average Reward 46.4]\n",
    "# ----------\n",
    "# [Episode 3630/4000] [Steps   53] [reward 54.0]\n",
    "# [Episode 3640/4000] [Steps   57] [reward 58.0]\n",
    "# [Episode 3650/4000] [Steps   68] [reward 69.0]\n",
    "# ----------\n",
    "# [TEST Episode 3650] [Average Reward 69.3]\n",
    "# ----------\n",
    "# [Episode 3660/4000] [Steps   64] [reward 65.0]\n",
    "# [Episode 3670/4000] [Steps   92] [reward 93.0]\n",
    "# ----------\n",
    "# [TEST Episode 3675] [Average Reward 91.1]\n",
    "# ----------\n",
    "# [Episode 3680/4000] [Steps   29] [reward 30.0]\n",
    "# [Episode 3690/4000] [Steps   78] [reward 79.0]\n",
    "# [Episode 3700/4000] [Steps  100] [reward 101.0]\n",
    "# ----------\n",
    "# [TEST Episode 3700] [Average Reward 108.8]\n",
    "# ----------\n",
    "# [Episode 3710/4000] [Steps   34] [reward 35.0]\n",
    "# [Episode 3720/4000] [Steps  136] [reward 137.0]\n",
    "# ----------\n",
    "# [TEST Episode 3725] [Average Reward 43.8]\n",
    "# ----------\n",
    "# [Episode 3730/4000] [Steps   60] [reward 61.0]\n",
    "# [Episode 3740/4000] [Steps   27] [reward 28.0]\n",
    "# [Episode 3750/4000] [Steps   60] [reward 61.0]\n",
    "# ----------\n",
    "# [TEST Episode 3750] [Average Reward 80.3]\n",
    "# ----------\n",
    "# [Episode 3760/4000] [Steps   58] [reward 59.0]\n",
    "# [Episode 3770/4000] [Steps  127] [reward 128.0]\n",
    "# ----------\n",
    "# [TEST Episode 3775] [Average Reward 134.3]\n",
    "# ----------\n",
    "# [Episode 3780/4000] [Steps   19] [reward 20.0]\n",
    "# [Episode 3790/4000] [Steps   62] [reward 63.0]\n",
    "# [Episode 3800/4000] [Steps  279] [reward 280.0]\n",
    "# ----------\n",
    "# [TEST Episode 3800] [Average Reward 140.1]\n",
    "# ----------\n",
    "# [Episode 3810/4000] [Steps   66] [reward 67.0]\n",
    "# [Episode 3820/4000] [Steps   71] [reward 72.0]\n",
    "# ----------\n",
    "# [TEST Episode 3825] [Average Reward 86.4]\n",
    "# ----------\n",
    "# [Episode 3830/4000] [Steps   61] [reward 62.0]\n",
    "# [Episode 3840/4000] [Steps  229] [reward 230.0]\n",
    "# [Episode 3850/4000] [Steps   78] [reward 79.0]\n",
    "# ----------\n",
    "# [TEST Episode 3850] [Average Reward 268.0]\n",
    "# ----------\n",
    "# [Episode 3860/4000] [Steps   30] [reward 31.0]\n",
    "# [Episode 3870/4000] [Steps   76] [reward 77.0]\n",
    "# ----------\n",
    "# [TEST Episode 3875] [Average Reward 151.6]\n",
    "# ----------\n",
    "# [Episode 3880/4000] [Steps   28] [reward 29.0]\n",
    "# [Episode 3890/4000] [Steps  229] [reward 230.0]\n",
    "# [Episode 3900/4000] [Steps   27] [reward 28.0]\n",
    "# ----------\n",
    "# [TEST Episode 3900] [Average Reward 106.0]\n",
    "# ----------\n",
    "# [Episode 3910/4000] [Steps   56] [reward 57.0]\n",
    "# [Episode 3920/4000] [Steps   77] [reward 78.0]\n",
    "# ----------\n",
    "# [TEST Episode 3925] [Average Reward 192.3]\n",
    "# ----------\n",
    "# [Episode 3930/4000] [Steps   29] [reward 30.0]\n",
    "# [Episode 3940/4000] [Steps  112] [reward 113.0]\n",
    "# [Episode 3950/4000] [Steps   12] [reward 13.0]\n",
    "# ----------\n",
    "# [TEST Episode 3950] [Average Reward 88.4]\n",
    "# ----------\n",
    "# [Episode 3960/4000] [Steps   40] [reward 41.0]\n",
    "# [Episode 3970/4000] [Steps   46] [reward 47.0]\n",
    "# ----------\n",
    "# [TEST Episode 3975] [Average Reward 153.7]\n",
    "# ----------\n",
    "# [Episode 3980/4000] [Steps   64] [reward 65.0]\n",
    "# [Episode 3990/4000] [Steps  117] [reward 118.0]\n",
    "# [Episode 4000/4000] [Steps  101] [reward 102.0]\n",
    "# ----------\n",
    "# [TEST Episode 4000] [Average Reward 111.7]\n",
    "# ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 [5 pts] Add the Experience Replay Buffer\n",
    "\n",
    "If you read the DQN paper (and as you can see from the algorithm picture above), the authors make use of an experience replay buffer to learn faster. We provide an implementation in the file `replay_buffer.py`. Update the `train_reinforcement_learning` code to push a tuple to the replay buffer and to sample a batch for the `optimize_model` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## PASTE YOUR TERMINAL OUTPUT HERE\n",
    "# # NOTE: TO HAVE LESS LINES PRINTED, YOU CAN SET THE VARIABLE PRINT_INTERVAL TO 5 or 10\n",
    "\n",
    "# [Episode   10/4000] [Steps   10] [reward 11.0]\n",
    "# [Episode   20/4000] [Steps   15] [reward 16.0]\n",
    "# ----------\n",
    "# saving model.\n",
    "# [TEST Episode 25] [Average Reward 8.9]\n",
    "# ----------\n",
    "# [Episode   30/4000] [Steps   12] [reward 13.0]\n",
    "# [Episode   40/4000] [Steps   11] [reward 12.0]\n",
    "# [Episode   50/4000] [Steps    9] [reward 10.0]\n",
    "# ----------\n",
    "# saving model.\n",
    "# [TEST Episode 50] [Average Reward 9.6]\n",
    "# ----------\n",
    "# [Episode   60/4000] [Steps    9] [reward 10.0]\n",
    "# [Episode   70/4000] [Steps   10] [reward 11.0]\n",
    "# ----------\n",
    "# saving model.\n",
    "# [TEST Episode 75] [Average Reward 9.8]\n",
    "# ----------\n",
    "# [Episode   80/4000] [Steps   10] [reward 11.0]\n",
    "# [Episode   90/4000] [Steps   10] [reward 11.0]\n",
    "# [Episode  100/4000] [Steps   10] [reward 11.0]\n",
    "# ----------\n",
    "# [TEST Episode 100] [Average Reward 9.6]\n",
    "# ----------\n",
    "# [Episode  110/4000] [Steps    8] [reward 9.0]\n",
    "# [Episode  120/4000] [Steps   12] [reward 13.0]\n",
    "# ----------\n",
    "# [TEST Episode 125] [Average Reward 9.1]\n",
    "# ----------\n",
    "# [Episode  130/4000] [Steps   11] [reward 12.0]\n",
    "# [Episode  140/4000] [Steps    8] [reward 9.0]\n",
    "# [Episode  150/4000] [Steps   11] [reward 12.0]\n",
    "# ----------\n",
    "# [TEST Episode 150] [Average Reward 9.6]\n",
    "# ----------\n",
    "# [Episode  160/4000] [Steps   10] [reward 11.0]\n",
    "# [Episode  170/4000] [Steps    9] [reward 10.0]\n",
    "# ----------\n",
    "# saving model.\n",
    "# [TEST Episode 175] [Average Reward 9.9]\n",
    "# ----------\n",
    "# [Episode  180/4000] [Steps   11] [reward 12.0]\n",
    "# [Episode  190/4000] [Steps   13] [reward 14.0]\n",
    "# [Episode  200/4000] [Steps   13] [reward 14.0]\n",
    "# ----------\n",
    "# saving model.\n",
    "# [TEST Episode 200] [Average Reward 55.6]\n",
    "# ----------\n",
    "# [Episode  210/4000] [Steps   10] [reward 11.0]\n",
    "# [Episode  220/4000] [Steps   10] [reward 11.0]\n",
    "# ----------\n",
    "# [TEST Episode 225] [Average Reward 10.3]\n",
    "# ----------\n",
    "# [Episode  230/4000] [Steps    8] [reward 9.0]\n",
    "# [Episode  240/4000] [Steps   11] [reward 12.0]\n",
    "# [Episode  250/4000] [Steps   41] [reward 42.0]\n",
    "# ----------\n",
    "# [TEST Episode 250] [Average Reward 21.4]\n",
    "# ----------\n",
    "# [Episode  260/4000] [Steps   19] [reward 20.0]\n",
    "# [Episode  270/4000] [Steps    9] [reward 10.0]\n",
    "# ----------\n",
    "# [TEST Episode 275] [Average Reward 38.2]\n",
    "# ----------\n",
    "# [Episode  280/4000] [Steps   23] [reward 24.0]\n",
    "# [Episode  290/4000] [Steps   14] [reward 15.0]\n",
    "# [Episode  300/4000] [Steps   10] [reward 11.0]\n",
    "# ----------\n",
    "# [TEST Episode 300] [Average Reward 10.9]\n",
    "# ----------\n",
    "# [Episode  310/4000] [Steps   12] [reward 13.0]\n",
    "# [Episode  320/4000] [Steps    9] [reward 10.0]\n",
    "# ----------\n",
    "# [TEST Episode 325] [Average Reward 9.8]\n",
    "# ----------\n",
    "# [Episode  330/4000] [Steps   11] [reward 12.0]\n",
    "# [Episode  340/4000] [Steps   22] [reward 23.0]\n",
    "# [Episode  350/4000] [Steps    9] [reward 10.0]\n",
    "# ----------\n",
    "# [TEST Episode 350] [Average Reward 12.1]\n",
    "# ----------\n",
    "# [Episode  360/4000] [Steps   14] [reward 15.0]\n",
    "# [Episode  370/4000] [Steps   26] [reward 27.0]\n",
    "# ----------\n",
    "# [TEST Episode 375] [Average Reward 12.6]\n",
    "# ----------\n",
    "# [Episode  380/4000] [Steps    9] [reward 10.0]\n",
    "# [Episode  390/4000] [Steps   91] [reward 92.0]\n",
    "# [Episode  400/4000] [Steps    8] [reward 9.0]\n",
    "# ----------\n",
    "# [TEST Episode 400] [Average Reward 11.6]\n",
    "# ----------\n",
    "# [Episode  410/4000] [Steps   10] [reward 11.0]\n",
    "# [Episode  420/4000] [Steps   15] [reward 16.0]\n",
    "# ----------\n",
    "# [TEST Episode 425] [Average Reward 14.0]\n",
    "# ----------\n",
    "# [Episode  430/4000] [Steps   76] [reward 77.0]\n",
    "# [Episode  440/4000] [Steps   13] [reward 14.0]\n",
    "# [Episode  450/4000] [Steps   15] [reward 16.0]\n",
    "# ----------\n",
    "# [TEST Episode 450] [Average Reward 14.2]\n",
    "# ----------\n",
    "# [Episode  460/4000] [Steps   10] [reward 11.0]\n",
    "# [Episode  470/4000] [Steps   13] [reward 14.0]\n",
    "# ----------\n",
    "# [TEST Episode 475] [Average Reward 45.6]\n",
    "# ----------\n",
    "# [Episode  480/4000] [Steps   22] [reward 23.0]\n",
    "# [Episode  490/4000] [Steps   20] [reward 21.0]\n",
    "# [Episode  500/4000] [Steps  119] [reward 120.0]\n",
    "# ----------\n",
    "# [TEST Episode 500] [Average Reward 13.6]\n",
    "# ----------\n",
    "# [Episode  510/4000] [Steps   69] [reward 70.0]\n",
    "# [Episode  520/4000] [Steps   23] [reward 24.0]\n",
    "# ----------\n",
    "# [TEST Episode 525] [Average Reward 24.8]\n",
    "# ----------\n",
    "# [Episode  530/4000] [Steps   18] [reward 19.0]\n",
    "# [Episode  540/4000] [Steps  321] [reward 322.0]\n",
    "# [Episode  550/4000] [Steps   34] [reward 35.0]\n",
    "# ----------\n",
    "# [TEST Episode 550] [Average Reward 10.9]\n",
    "# ----------\n",
    "# [Episode  560/4000] [Steps    9] [reward 10.0]\n",
    "# [Episode  570/4000] [Steps    8] [reward 9.0]\n",
    "# ----------\n",
    "# [TEST Episode 575] [Average Reward 9.4]\n",
    "# ----------\n",
    "# [Episode  580/4000] [Steps  180] [reward 181.0]\n",
    "# [Episode  590/4000] [Steps    8] [reward 9.0]\n",
    "# [Episode  600/4000] [Steps   12] [reward 13.0]\n",
    "# ----------\n",
    "# [TEST Episode 600] [Average Reward 12.4]\n",
    "# ----------\n",
    "# [Episode  610/4000] [Steps  321] [reward 322.0]\n",
    "# [Episode  620/4000] [Steps  147] [reward 148.0]\n",
    "# ----------\n",
    "# saving model.\n",
    "# [TEST Episode 625] [Average Reward 95.6]\n",
    "# ----------\n",
    "# [Episode  630/4000] [Steps   13] [reward 14.0]\n",
    "# [Episode  640/4000] [Steps  252] [reward 253.0]\n",
    "# [Episode  650/4000] [Steps   23] [reward 24.0]\n",
    "# ----------\n",
    "# [TEST Episode 650] [Average Reward 26.6]\n",
    "# ----------\n",
    "# [Episode  660/4000] [Steps   17] [reward 18.0]\n",
    "# [Episode  670/4000] [Steps   15] [reward 16.0]\n",
    "# ----------\n",
    "# [TEST Episode 675] [Average Reward 20.9]\n",
    "# ----------\n",
    "# [Episode  680/4000] [Steps   64] [reward 65.0]\n",
    "# [Episode  690/4000] [Steps   40] [reward 41.0]\n",
    "# [Episode  700/4000] [Steps   11] [reward 12.0]\n",
    "# ----------\n",
    "# [TEST Episode 700] [Average Reward 59.4]\n",
    "# ----------\n",
    "# [Episode  710/4000] [Steps  117] [reward 118.0]\n",
    "# [Episode  720/4000] [Steps  221] [reward 222.0]\n",
    "# ----------\n",
    "# saving model.\n",
    "# [TEST Episode 725] [Average Reward 295.2]\n",
    "# ----------\n",
    "# [Episode  730/4000] [Steps  499] [reward 500.0]\n",
    "# [Episode  740/4000] [Steps   12] [reward 13.0]\n",
    "# [Episode  750/4000] [Steps  139] [reward 140.0]\n",
    "# ----------\n",
    "# [TEST Episode 750] [Average Reward 288.5]\n",
    "# ----------\n",
    "# [Episode  760/4000] [Steps   22] [reward 23.0]\n",
    "# [Episode  770/4000] [Steps   11] [reward 12.0]\n",
    "# ----------\n",
    "# [TEST Episode 775] [Average Reward 100.2]\n",
    "# ----------\n",
    "# [Episode  780/4000] [Steps   47] [reward 48.0]\n",
    "# [Episode  790/4000] [Steps   62] [reward 63.0]\n",
    "# [Episode  800/4000] [Steps   13] [reward 14.0]\n",
    "# ----------\n",
    "# [TEST Episode 800] [Average Reward 29.6]\n",
    "# ----------\n",
    "# [Episode  810/4000] [Steps  164] [reward 165.0]\n",
    "# [Episode  820/4000] [Steps  134] [reward 135.0]\n",
    "# ----------\n",
    "# [TEST Episode 825] [Average Reward 283.5]\n",
    "# ----------\n",
    "# [Episode  830/4000] [Steps  228] [reward 229.0]\n",
    "# [Episode  840/4000] [Steps  153] [reward 154.0]\n",
    "# [Episode  850/4000] [Steps  127] [reward 128.0]\n",
    "# ----------\n",
    "# [TEST Episode 850] [Average Reward 144.9]\n",
    "# ----------\n",
    "# [Episode  860/4000] [Steps   95] [reward 96.0]\n",
    "# [Episode  870/4000] [Steps  110] [reward 111.0]\n",
    "# ----------\n",
    "# [TEST Episode 875] [Average Reward 169.3]\n",
    "# ----------\n",
    "# [Episode  880/4000] [Steps   15] [reward 16.0]\n",
    "# [Episode  890/4000] [Steps  204] [reward 205.0]\n",
    "# [Episode  900/4000] [Steps   10] [reward 11.0]\n",
    "# ----------\n",
    "# [TEST Episode 900] [Average Reward 120.7]\n",
    "# ----------\n",
    "# [Episode  910/4000] [Steps   61] [reward 62.0]\n",
    "# [Episode  920/4000] [Steps  152] [reward 153.0]\n",
    "# ----------\n",
    "# saving model.\n",
    "# [TEST Episode 925] [Average Reward 452.2]\n",
    "# ----------\n",
    "# [Episode  930/4000] [Steps   10] [reward 11.0]\n",
    "# [Episode  940/4000] [Steps  347] [reward 348.0]\n",
    "# [Episode  950/4000] [Steps  499] [reward 500.0]\n",
    "# ----------\n",
    "# [TEST Episode 950] [Average Reward 228.4]\n",
    "# ----------\n",
    "# [Episode  960/4000] [Steps  499] [reward 500.0]\n",
    "# [Episode  970/4000] [Steps  335] [reward 336.0]\n",
    "# ----------\n",
    "# [TEST Episode 975] [Average Reward 352.2]\n",
    "# ----------\n",
    "# [Episode  980/4000] [Steps  220] [reward 221.0]\n",
    "# [Episode  990/4000] [Steps  499] [reward 500.0]\n",
    "# [Episode 1000/4000] [Steps  499] [reward 500.0]\n",
    "# ----------\n",
    "# saving model.\n",
    "# [TEST Episode 1000] [Average Reward 500.0]\n",
    "# ----------\n",
    "# [Episode 1010/4000] [Steps  124] [reward 125.0]\n",
    "# [Episode 1020/4000] [Steps    9] [reward 10.0]\n",
    "# ----------\n",
    "# [TEST Episode 1025] [Average Reward 37.1]\n",
    "# ----------\n",
    "# [Episode 1030/4000] [Steps   94] [reward 95.0]\n",
    "# [Episode 1040/4000] [Steps    8] [reward 9.0]\n",
    "# [Episode 1050/4000] [Steps  157] [reward 158.0]\n",
    "# ----------\n",
    "# [TEST Episode 1050] [Average Reward 412.7]\n",
    "# ----------\n",
    "# [Episode 1060/4000] [Steps   14] [reward 15.0]\n",
    "# [Episode 1070/4000] [Steps   82] [reward 83.0]\n",
    "# ----------\n",
    "# [TEST Episode 1075] [Average Reward 131.0]\n",
    "# ----------\n",
    "# [Episode 1080/4000] [Steps   90] [reward 91.0]\n",
    "# [Episode 1090/4000] [Steps  126] [reward 127.0]\n",
    "# [Episode 1100/4000] [Steps   11] [reward 12.0]\n",
    "# ----------\n",
    "# [TEST Episode 1100] [Average Reward 9.6]\n",
    "# ----------\n",
    "# [Episode 1110/4000] [Steps   18] [reward 19.0]\n",
    "# [Episode 1120/4000] [Steps  176] [reward 177.0]\n",
    "# ----------\n",
    "# [TEST Episode 1125] [Average Reward 315.3]\n",
    "# ----------\n",
    "# [Episode 1130/4000] [Steps  131] [reward 132.0]\n",
    "# [Episode 1140/4000] [Steps  128] [reward 129.0]\n",
    "# [Episode 1150/4000] [Steps  214] [reward 215.0]\n",
    "# ----------\n",
    "# [TEST Episode 1150] [Average Reward 349.6]\n",
    "# ----------\n",
    "# [Episode 1160/4000] [Steps   11] [reward 12.0]\n",
    "# [Episode 1170/4000] [Steps   11] [reward 12.0]\n",
    "# ----------\n",
    "# [TEST Episode 1175] [Average Reward 316.4]\n",
    "# ----------\n",
    "# [Episode 1180/4000] [Steps  252] [reward 253.0]\n",
    "# [Episode 1190/4000] [Steps  270] [reward 271.0]\n",
    "# [Episode 1200/4000] [Steps  135] [reward 136.0]\n",
    "# ----------\n",
    "# [TEST Episode 1200] [Average Reward 174.3]\n",
    "# ----------\n",
    "# [Episode 1210/4000] [Steps  185] [reward 186.0]\n",
    "# [Episode 1220/4000] [Steps   12] [reward 13.0]\n",
    "# ----------\n",
    "# [TEST Episode 1225] [Average Reward 110.6]\n",
    "# ----------\n",
    "# [Episode 1230/4000] [Steps  213] [reward 214.0]\n",
    "# [Episode 1240/4000] [Steps  240] [reward 241.0]\n",
    "# [Episode 1250/4000] [Steps    7] [reward 8.0]\n",
    "# ----------\n",
    "# [TEST Episode 1250] [Average Reward 10.7]\n",
    "# ----------\n",
    "# [Episode 1260/4000] [Steps   11] [reward 12.0]\n",
    "# [Episode 1270/4000] [Steps   61] [reward 62.0]\n",
    "# ----------\n",
    "# [TEST Episode 1275] [Average Reward 11.5]\n",
    "# ----------\n",
    "# [Episode 1280/4000] [Steps   14] [reward 15.0]\n",
    "# [Episode 1290/4000] [Steps  127] [reward 128.0]\n",
    "# [Episode 1300/4000] [Steps  389] [reward 390.0]\n",
    "# ----------\n",
    "# [TEST Episode 1300] [Average Reward 71.3]\n",
    "# ----------\n",
    "# [Episode 1310/4000] [Steps  175] [reward 176.0]\n",
    "# [Episode 1320/4000] [Steps  499] [reward 500.0]\n",
    "# ----------\n",
    "# [TEST Episode 1325] [Average Reward 200.5]\n",
    "# ----------\n",
    "# [Episode 1330/4000] [Steps  188] [reward 189.0]\n",
    "# [Episode 1340/4000] [Steps  209] [reward 210.0]\n",
    "# [Episode 1350/4000] [Steps    9] [reward 10.0]\n",
    "# ----------\n",
    "# [TEST Episode 1350] [Average Reward 15.2]\n",
    "# ----------\n",
    "# [Episode 1360/4000] [Steps  293] [reward 294.0]\n",
    "# [Episode 1370/4000] [Steps   18] [reward 19.0]\n",
    "# ----------\n",
    "# [TEST Episode 1375] [Average Reward 293.9]\n",
    "# ----------\n",
    "# [Episode 1380/4000] [Steps  186] [reward 187.0]\n",
    "# [Episode 1390/4000] [Steps   42] [reward 43.0]\n",
    "# [Episode 1400/4000] [Steps   71] [reward 72.0]\n",
    "# ----------\n",
    "# [TEST Episode 1400] [Average Reward 61.6]\n",
    "# ----------\n",
    "# [Episode 1410/4000] [Steps  179] [reward 180.0]\n",
    "# [Episode 1420/4000] [Steps  142] [reward 143.0]\n",
    "# ----------\n",
    "# [TEST Episode 1425] [Average Reward 191.0]\n",
    "# ----------\n",
    "# [Episode 1430/4000] [Steps  159] [reward 160.0]\n",
    "# [Episode 1440/4000] [Steps    9] [reward 10.0]\n",
    "# [Episode 1450/4000] [Steps  373] [reward 374.0]\n",
    "# ----------\n",
    "# [TEST Episode 1450] [Average Reward 197.7]\n",
    "# ----------\n",
    "# [Episode 1460/4000] [Steps   12] [reward 13.0]\n",
    "# [Episode 1470/4000] [Steps  219] [reward 220.0]\n",
    "# ----------\n",
    "# [TEST Episode 1475] [Average Reward 190.3]\n",
    "# ----------\n",
    "# [Episode 1480/4000] [Steps  173] [reward 174.0]\n",
    "# [Episode 1490/4000] [Steps  302] [reward 303.0]\n",
    "# [Episode 1500/4000] [Steps  168] [reward 169.0]\n",
    "# ----------\n",
    "# [TEST Episode 1500] [Average Reward 334.1]\n",
    "# ----------\n",
    "# [Episode 1510/4000] [Steps   11] [reward 12.0]\n",
    "# [Episode 1520/4000] [Steps   24] [reward 25.0]\n",
    "# ----------\n",
    "# [TEST Episode 1525] [Average Reward 23.3]\n",
    "# ----------\n",
    "# [Episode 1530/4000] [Steps  139] [reward 140.0]\n",
    "# [Episode 1540/4000] [Steps   19] [reward 20.0]\n",
    "# [Episode 1550/4000] [Steps   10] [reward 11.0]\n",
    "# ----------\n",
    "# [TEST Episode 1550] [Average Reward 13.1]\n",
    "# ----------\n",
    "# [Episode 1560/4000] [Steps   11] [reward 12.0]\n",
    "# [Episode 1570/4000] [Steps  121] [reward 122.0]\n",
    "# ----------\n",
    "# [TEST Episode 1575] [Average Reward 172.2]\n",
    "# ----------\n",
    "# [Episode 1580/4000] [Steps   19] [reward 20.0]\n",
    "# [Episode 1590/4000] [Steps   17] [reward 18.0]\n",
    "# [Episode 1600/4000] [Steps   87] [reward 88.0]\n",
    "# ----------\n",
    "# [TEST Episode 1600] [Average Reward 209.3]\n",
    "# ----------\n",
    "# [Episode 1610/4000] [Steps   28] [reward 29.0]\n",
    "# [Episode 1620/4000] [Steps  127] [reward 128.0]\n",
    "# ----------\n",
    "# [TEST Episode 1625] [Average Reward 459.5]\n",
    "# ----------\n",
    "# [Episode 1630/4000] [Steps  270] [reward 271.0]\n",
    "# [Episode 1640/4000] [Steps    8] [reward 9.0]\n",
    "# [Episode 1650/4000] [Steps    9] [reward 10.0]\n",
    "# ----------\n",
    "# [TEST Episode 1650] [Average Reward 344.9]\n",
    "# ----------\n",
    "# [Episode 1660/4000] [Steps   94] [reward 95.0]\n",
    "# [Episode 1670/4000] [Steps  203] [reward 204.0]\n",
    "# ----------\n",
    "# [TEST Episode 1675] [Average Reward 112.1]\n",
    "# ----------\n",
    "# [Episode 1680/4000] [Steps  129] [reward 130.0]\n",
    "# [Episode 1690/4000] [Steps    8] [reward 9.0]\n",
    "# [Episode 1700/4000] [Steps  211] [reward 212.0]\n",
    "# ----------\n",
    "# [TEST Episode 1700] [Average Reward 210.4]\n",
    "# ----------\n",
    "# [Episode 1710/4000] [Steps  116] [reward 117.0]\n",
    "# [Episode 1720/4000] [Steps    8] [reward 9.0]\n",
    "# ----------\n",
    "# [TEST Episode 1725] [Average Reward 220.0]\n",
    "# ----------\n",
    "# [Episode 1730/4000] [Steps  193] [reward 194.0]\n",
    "# [Episode 1740/4000] [Steps    8] [reward 9.0]\n",
    "# [Episode 1750/4000] [Steps   10] [reward 11.0]\n",
    "# ----------\n",
    "# [TEST Episode 1750] [Average Reward 15.1]\n",
    "# ----------\n",
    "# [Episode 1760/4000] [Steps  166] [reward 167.0]\n",
    "# [Episode 1770/4000] [Steps   55] [reward 56.0]\n",
    "# ----------\n",
    "# [TEST Episode 1775] [Average Reward 69.3]\n",
    "# ----------\n",
    "# [Episode 1780/4000] [Steps   14] [reward 15.0]\n",
    "# [Episode 1790/4000] [Steps  184] [reward 185.0]\n",
    "# [Episode 1800/4000] [Steps  200] [reward 201.0]\n",
    "# ----------\n",
    "# [TEST Episode 1800] [Average Reward 104.2]\n",
    "# ----------\n",
    "# [Episode 1810/4000] [Steps   44] [reward 45.0]\n",
    "# [Episode 1820/4000] [Steps    8] [reward 9.0]\n",
    "# ----------\n",
    "# [TEST Episode 1825] [Average Reward 256.7]\n",
    "# ----------\n",
    "# [Episode 1830/4000] [Steps  115] [reward 116.0]\n",
    "# [Episode 1840/4000] [Steps   96] [reward 97.0]\n",
    "# [Episode 1850/4000] [Steps   60] [reward 61.0]\n",
    "# ----------\n",
    "# [TEST Episode 1850] [Average Reward 176.4]\n",
    "# ----------\n",
    "# [Episode 1860/4000] [Steps  217] [reward 218.0]\n",
    "# [Episode 1870/4000] [Steps  128] [reward 129.0]\n",
    "# ----------\n",
    "# [TEST Episode 1875] [Average Reward 9.3]\n",
    "# ----------\n",
    "# [Episode 1880/4000] [Steps   20] [reward 21.0]\n",
    "# [Episode 1890/4000] [Steps   62] [reward 63.0]\n",
    "# [Episode 1900/4000] [Steps    8] [reward 9.0]\n",
    "# ----------\n",
    "# [TEST Episode 1900] [Average Reward 10.3]\n",
    "# ----------\n",
    "# [Episode 1910/4000] [Steps  193] [reward 194.0]\n",
    "# [Episode 1920/4000] [Steps    9] [reward 10.0]\n",
    "# ----------\n",
    "# [TEST Episode 1925] [Average Reward 112.2]\n",
    "# ----------\n",
    "# [Episode 1930/4000] [Steps  119] [reward 120.0]\n",
    "# [Episode 1940/4000] [Steps   71] [reward 72.0]\n",
    "# [Episode 1950/4000] [Steps  204] [reward 205.0]\n",
    "# ----------\n",
    "# [TEST Episode 1950] [Average Reward 215.0]\n",
    "# ----------\n",
    "# [Episode 1960/4000] [Steps   13] [reward 14.0]\n",
    "# [Episode 1970/4000] [Steps  184] [reward 185.0]\n",
    "# ----------\n",
    "# [TEST Episode 1975] [Average Reward 235.1]\n",
    "# ----------\n",
    "# [Episode 1980/4000] [Steps  231] [reward 232.0]\n",
    "# [Episode 1990/4000] [Steps   10] [reward 11.0]\n",
    "# [Episode 2000/4000] [Steps    7] [reward 8.0]\n",
    "# ----------\n",
    "# [TEST Episode 2000] [Average Reward 258.7]\n",
    "# ----------\n",
    "# [Episode 2010/4000] [Steps   11] [reward 12.0]\n",
    "# [Episode 2020/4000] [Steps    9] [reward 10.0]\n",
    "# ----------\n",
    "# [TEST Episode 2025] [Average Reward 500.0]\n",
    "# ----------\n",
    "# [Episode 2030/4000] [Steps   41] [reward 42.0]\n",
    "# [Episode 2040/4000] [Steps    8] [reward 9.0]\n",
    "# [Episode 2050/4000] [Steps   42] [reward 43.0]\n",
    "# ----------\n",
    "# [TEST Episode 2050] [Average Reward 383.3]\n",
    "# ----------\n",
    "# [Episode 2060/4000] [Steps   29] [reward 30.0]\n",
    "# [Episode 2070/4000] [Steps  117] [reward 118.0]\n",
    "# ----------\n",
    "# [TEST Episode 2075] [Average Reward 17.9]\n",
    "# ----------\n",
    "# [Episode 2080/4000] [Steps   88] [reward 89.0]\n",
    "# [Episode 2090/4000] [Steps  178] [reward 179.0]\n",
    "# [Episode 2100/4000] [Steps   13] [reward 14.0]\n",
    "# ----------\n",
    "# [TEST Episode 2100] [Average Reward 68.0]\n",
    "# ----------\n",
    "# [Episode 2110/4000] [Steps    7] [reward 8.0]\n",
    "# [Episode 2120/4000] [Steps  195] [reward 196.0]\n",
    "# ----------\n",
    "# [TEST Episode 2125] [Average Reward 117.7]\n",
    "# ----------\n",
    "# [Episode 2130/4000] [Steps  124] [reward 125.0]\n",
    "# [Episode 2140/4000] [Steps  245] [reward 246.0]\n",
    "# [Episode 2150/4000] [Steps  499] [reward 500.0]\n",
    "# ----------\n",
    "# [TEST Episode 2150] [Average Reward 485.8]\n",
    "# ----------\n",
    "# [Episode 2160/4000] [Steps   11] [reward 12.0]\n",
    "# [Episode 2170/4000] [Steps    9] [reward 10.0]\n",
    "# ----------\n",
    "# [TEST Episode 2175] [Average Reward 166.7]\n",
    "# ----------\n",
    "# [Episode 2180/4000] [Steps   87] [reward 88.0]\n",
    "# [Episode 2190/4000] [Steps   10] [reward 11.0]\n",
    "# [Episode 2200/4000] [Steps  140] [reward 141.0]\n",
    "# ----------\n",
    "# [TEST Episode 2200] [Average Reward 245.4]\n",
    "# ----------\n",
    "# [Episode 2210/4000] [Steps   91] [reward 92.0]\n",
    "# [Episode 2220/4000] [Steps    8] [reward 9.0]\n",
    "# ----------\n",
    "# [TEST Episode 2225] [Average Reward 85.1]\n",
    "# ----------\n",
    "# [Episode 2230/4000] [Steps  196] [reward 197.0]\n",
    "# [Episode 2240/4000] [Steps   11] [reward 12.0]\n",
    "# [Episode 2250/4000] [Steps   11] [reward 12.0]\n",
    "# ----------\n",
    "# [TEST Episode 2250] [Average Reward 9.2]\n",
    "# ----------\n",
    "# [Episode 2260/4000] [Steps   69] [reward 70.0]\n",
    "# [Episode 2270/4000] [Steps   13] [reward 14.0]\n",
    "# ----------\n",
    "# [TEST Episode 2275] [Average Reward 62.1]\n",
    "# ----------\n",
    "# [Episode 2280/4000] [Steps   52] [reward 53.0]\n",
    "# [Episode 2290/4000] [Steps   60] [reward 61.0]\n",
    "# [Episode 2300/4000] [Steps   26] [reward 27.0]\n",
    "# ----------\n",
    "# [TEST Episode 2300] [Average Reward 53.8]\n",
    "# ----------\n",
    "# [Episode 2310/4000] [Steps   33] [reward 34.0]\n",
    "# [Episode 2320/4000] [Steps  211] [reward 212.0]\n",
    "# ----------\n",
    "# [TEST Episode 2325] [Average Reward 179.9]\n",
    "# ----------\n",
    "# [Episode 2330/4000] [Steps   50] [reward 51.0]\n",
    "# [Episode 2340/4000] [Steps  191] [reward 192.0]\n",
    "# [Episode 2350/4000] [Steps   44] [reward 45.0]\n",
    "# ----------\n",
    "# [TEST Episode 2350] [Average Reward 127.3]\n",
    "# ----------\n",
    "# [Episode 2360/4000] [Steps    8] [reward 9.0]\n",
    "# [Episode 2370/4000] [Steps  221] [reward 222.0]\n",
    "# ----------\n",
    "# [TEST Episode 2375] [Average Reward 91.0]\n",
    "# ----------\n",
    "# [Episode 2380/4000] [Steps  187] [reward 188.0]\n",
    "# [Episode 2390/4000] [Steps  221] [reward 222.0]\n",
    "# [Episode 2400/4000] [Steps  261] [reward 262.0]\n",
    "# ----------\n",
    "# [TEST Episode 2400] [Average Reward 55.9]\n",
    "# ----------\n",
    "# [Episode 2410/4000] [Steps  499] [reward 500.0]\n",
    "# [Episode 2420/4000] [Steps   13] [reward 14.0]\n",
    "# ----------\n",
    "# [TEST Episode 2425] [Average Reward 11.0]\n",
    "# ----------\n",
    "# [Episode 2430/4000] [Steps   36] [reward 37.0]\n",
    "# [Episode 2440/4000] [Steps   25] [reward 26.0]\n",
    "# [Episode 2450/4000] [Steps  107] [reward 108.0]\n",
    "# ----------\n",
    "# [TEST Episode 2450] [Average Reward 500.0]\n",
    "# ----------\n",
    "# [Episode 2460/4000] [Steps  142] [reward 143.0]\n",
    "# [Episode 2470/4000] [Steps  499] [reward 500.0]\n",
    "# ----------\n",
    "# [TEST Episode 2475] [Average Reward 500.0]\n",
    "# ----------\n",
    "# [Episode 2480/4000] [Steps   72] [reward 73.0]\n",
    "# [Episode 2490/4000] [Steps  499] [reward 500.0]\n",
    "# [Episode 2500/4000] [Steps  383] [reward 384.0]\n",
    "# ----------\n",
    "# [TEST Episode 2500] [Average Reward 319.4]\n",
    "# ----------\n",
    "# [Episode 2510/4000] [Steps  126] [reward 127.0]\n",
    "# [Episode 2520/4000] [Steps  411] [reward 412.0]\n",
    "# ----------\n",
    "# [TEST Episode 2525] [Average Reward 85.8]\n",
    "# ----------\n",
    "# [Episode 2530/4000] [Steps   12] [reward 13.0]\n",
    "# [Episode 2540/4000] [Steps    9] [reward 10.0]\n",
    "# [Episode 2550/4000] [Steps   25] [reward 26.0]\n",
    "# ----------\n",
    "# [TEST Episode 2550] [Average Reward 314.8]\n",
    "# ----------\n",
    "# [Episode 2560/4000] [Steps  147] [reward 148.0]\n",
    "# [Episode 2570/4000] [Steps  166] [reward 167.0]\n",
    "# ----------\n",
    "# [TEST Episode 2575] [Average Reward 76.4]\n",
    "# ----------\n",
    "# [Episode 2580/4000] [Steps  111] [reward 112.0]\n",
    "# [Episode 2590/4000] [Steps   31] [reward 32.0]\n",
    "# [Episode 2600/4000] [Steps  103] [reward 104.0]\n",
    "# ----------\n",
    "# [TEST Episode 2600] [Average Reward 96.8]\n",
    "# ----------\n",
    "# [Episode 2610/4000] [Steps  499] [reward 500.0]\n",
    "# [Episode 2620/4000] [Steps  213] [reward 214.0]\n",
    "# ----------\n",
    "# [TEST Episode 2625] [Average Reward 87.1]\n",
    "# ----------\n",
    "# [Episode 2630/4000] [Steps  472] [reward 473.0]\n",
    "# [Episode 2640/4000] [Steps  499] [reward 500.0]\n",
    "# [Episode 2650/4000] [Steps    9] [reward 10.0]\n",
    "# ----------\n",
    "# [TEST Episode 2650] [Average Reward 10.5]\n",
    "# ----------\n",
    "# [Episode 2660/4000] [Steps   10] [reward 11.0]\n",
    "# [Episode 2670/4000] [Steps  150] [reward 151.0]\n",
    "# ----------\n",
    "# [TEST Episode 2675] [Average Reward 94.9]\n",
    "# ----------\n",
    "# [Episode 2680/4000] [Steps  171] [reward 172.0]\n",
    "# [Episode 2690/4000] [Steps    8] [reward 9.0]\n",
    "# [Episode 2700/4000] [Steps   91] [reward 92.0]\n",
    "# ----------\n",
    "# [TEST Episode 2700] [Average Reward 77.7]\n",
    "# ----------\n",
    "# [Episode 2710/4000] [Steps   19] [reward 20.0]\n",
    "# [Episode 2720/4000] [Steps   88] [reward 89.0]\n",
    "# ----------\n",
    "# [TEST Episode 2725] [Average Reward 500.0]\n",
    "# ----------\n",
    "# [Episode 2730/4000] [Steps   29] [reward 30.0]\n",
    "# [Episode 2740/4000] [Steps   83] [reward 84.0]\n",
    "# [Episode 2750/4000] [Steps    8] [reward 9.0]\n",
    "# ----------\n",
    "# [TEST Episode 2750] [Average Reward 10.0]\n",
    "# ----------\n",
    "# [Episode 2760/4000] [Steps   70] [reward 71.0]\n",
    "# [Episode 2770/4000] [Steps   11] [reward 12.0]\n",
    "# ----------\n",
    "# [TEST Episode 2775] [Average Reward 37.4]\n",
    "# ----------\n",
    "# [Episode 2780/4000] [Steps   49] [reward 50.0]\n",
    "# [Episode 2790/4000] [Steps   76] [reward 77.0]\n",
    "# [Episode 2800/4000] [Steps  147] [reward 148.0]\n",
    "# ----------\n",
    "# [TEST Episode 2800] [Average Reward 94.7]\n",
    "# ----------\n",
    "# [Episode 2810/4000] [Steps   46] [reward 47.0]\n",
    "# [Episode 2820/4000] [Steps   59] [reward 60.0]\n",
    "# ----------\n",
    "# [TEST Episode 2825] [Average Reward 121.7]\n",
    "# ----------\n",
    "# [Episode 2830/4000] [Steps  130] [reward 131.0]\n",
    "# [Episode 2840/4000] [Steps  140] [reward 141.0]\n",
    "# [Episode 2850/4000] [Steps   19] [reward 20.0]\n",
    "# ----------\n",
    "# [TEST Episode 2850] [Average Reward 213.2]\n",
    "# ----------\n",
    "# [Episode 2860/4000] [Steps  256] [reward 257.0]\n",
    "# [Episode 2870/4000] [Steps  358] [reward 359.0]\n",
    "# ----------\n",
    "# [TEST Episode 2875] [Average Reward 251.2]\n",
    "# ----------\n",
    "# [Episode 2880/4000] [Steps  154] [reward 155.0]\n",
    "# [Episode 2890/4000] [Steps    9] [reward 10.0]\n",
    "# [Episode 2900/4000] [Steps   35] [reward 36.0]\n",
    "# ----------\n",
    "# [TEST Episode 2900] [Average Reward 94.7]\n",
    "# ----------\n",
    "# [Episode 2910/4000] [Steps  368] [reward 369.0]\n",
    "# [Episode 2920/4000] [Steps  166] [reward 167.0]\n",
    "# ----------\n",
    "# [TEST Episode 2925] [Average Reward 286.8]\n",
    "# ----------\n",
    "# [Episode 2930/4000] [Steps   16] [reward 17.0]\n",
    "# [Episode 2940/4000] [Steps  130] [reward 131.0]\n",
    "# [Episode 2950/4000] [Steps  102] [reward 103.0]\n",
    "# ----------\n",
    "# [TEST Episode 2950] [Average Reward 95.1]\n",
    "# ----------\n",
    "# [Episode 2960/4000] [Steps   11] [reward 12.0]\n",
    "# [Episode 2970/4000] [Steps  136] [reward 137.0]\n",
    "# ----------\n",
    "# [TEST Episode 2975] [Average Reward 111.7]\n",
    "# ----------\n",
    "# [Episode 2980/4000] [Steps  105] [reward 106.0]\n",
    "# [Episode 2990/4000] [Steps   13] [reward 14.0]\n",
    "# [Episode 3000/4000] [Steps  128] [reward 129.0]\n",
    "# ----------\n",
    "# [TEST Episode 3000] [Average Reward 135.3]\n",
    "# ----------\n",
    "# [Episode 3010/4000] [Steps  112] [reward 113.0]\n",
    "# [Episode 3020/4000] [Steps   33] [reward 34.0]\n",
    "# ----------\n",
    "# [TEST Episode 3025] [Average Reward 162.2]\n",
    "# ----------\n",
    "# [Episode 3030/4000] [Steps  499] [reward 500.0]\n",
    "# [Episode 3040/4000] [Steps  175] [reward 176.0]\n",
    "# [Episode 3050/4000] [Steps  374] [reward 375.0]\n",
    "# ----------\n",
    "# [TEST Episode 3050] [Average Reward 177.3]\n",
    "# ----------\n",
    "# [Episode 3060/4000] [Steps  229] [reward 230.0]\n",
    "# [Episode 3070/4000] [Steps  150] [reward 151.0]\n",
    "# ----------\n",
    "# [TEST Episode 3075] [Average Reward 9.7]\n",
    "# ----------\n",
    "# [Episode 3080/4000] [Steps   16] [reward 17.0]\n",
    "# [Episode 3090/4000] [Steps   34] [reward 35.0]\n",
    "# [Episode 3100/4000] [Steps   49] [reward 50.0]\n",
    "# ----------\n",
    "# [TEST Episode 3100] [Average Reward 74.0]\n",
    "# ----------\n",
    "# [Episode 3110/4000] [Steps  225] [reward 226.0]\n",
    "# [Episode 3120/4000] [Steps  115] [reward 116.0]\n",
    "# ----------\n",
    "# [TEST Episode 3125] [Average Reward 60.3]\n",
    "# ----------\n",
    "# [Episode 3130/4000] [Steps   57] [reward 58.0]\n",
    "# [Episode 3140/4000] [Steps   99] [reward 100.0]\n",
    "# [Episode 3150/4000] [Steps  101] [reward 102.0]\n",
    "# ----------\n",
    "# [TEST Episode 3150] [Average Reward 130.3]\n",
    "# ----------\n",
    "# [Episode 3160/4000] [Steps   12] [reward 13.0]\n",
    "# [Episode 3170/4000] [Steps  173] [reward 174.0]\n",
    "# ----------\n",
    "# [TEST Episode 3175] [Average Reward 97.3]\n",
    "# ----------\n",
    "# [Episode 3180/4000] [Steps   81] [reward 82.0]\n",
    "# [Episode 3190/4000] [Steps  249] [reward 250.0]\n",
    "# [Episode 3200/4000] [Steps   92] [reward 93.0]\n",
    "# ----------\n",
    "# [TEST Episode 3200] [Average Reward 180.0]\n",
    "# ----------\n",
    "# [Episode 3210/4000] [Steps   81] [reward 82.0]\n",
    "# [Episode 3220/4000] [Steps   40] [reward 41.0]\n",
    "# ----------\n",
    "# [TEST Episode 3225] [Average Reward 122.9]\n",
    "# ----------\n",
    "# [Episode 3230/4000] [Steps  127] [reward 128.0]\n",
    "# [Episode 3240/4000] [Steps  127] [reward 128.0]\n",
    "# [Episode 3250/4000] [Steps   52] [reward 53.0]\n",
    "# ----------\n",
    "# [TEST Episode 3250] [Average Reward 141.7]\n",
    "# ----------\n",
    "# [Episode 3260/4000] [Steps  251] [reward 252.0]\n",
    "# [Episode 3270/4000] [Steps  182] [reward 183.0]\n",
    "# ----------\n",
    "# [TEST Episode 3275] [Average Reward 159.5]\n",
    "# ----------\n",
    "# [Episode 3280/4000] [Steps  171] [reward 172.0]\n",
    "# [Episode 3290/4000] [Steps  478] [reward 479.0]\n",
    "# [Episode 3300/4000] [Steps  499] [reward 500.0]\n",
    "# ----------\n",
    "# [TEST Episode 3300] [Average Reward 500.0]\n",
    "# ----------\n",
    "# [Episode 3310/4000] [Steps  179] [reward 180.0]\n",
    "# [Episode 3320/4000] [Steps   11] [reward 12.0]\n",
    "# ----------\n",
    "# [TEST Episode 3325] [Average Reward 207.2]\n",
    "# ----------\n",
    "# [Episode 3330/4000] [Steps  499] [reward 500.0]\n",
    "# [Episode 3340/4000] [Steps   92] [reward 93.0]\n",
    "# [Episode 3350/4000] [Steps   38] [reward 39.0]\n",
    "# ----------\n",
    "# [TEST Episode 3350] [Average Reward 500.0]\n",
    "# ----------\n",
    "# [Episode 3360/4000] [Steps   41] [reward 42.0]\n",
    "# [Episode 3370/4000] [Steps  218] [reward 219.0]\n",
    "# ----------\n",
    "# [TEST Episode 3375] [Average Reward 160.9]\n",
    "# ----------\n",
    "# [Episode 3380/4000] [Steps  202] [reward 203.0]\n",
    "# [Episode 3390/4000] [Steps  499] [reward 500.0]\n",
    "# [Episode 3400/4000] [Steps  197] [reward 198.0]\n",
    "# ----------\n",
    "# [TEST Episode 3400] [Average Reward 98.8]\n",
    "# ----------\n",
    "# [Episode 3410/4000] [Steps   55] [reward 56.0]\n",
    "# [Episode 3420/4000] [Steps  499] [reward 500.0]\n",
    "# ----------\n",
    "# [TEST Episode 3425] [Average Reward 124.9]\n",
    "# ----------\n",
    "# [Episode 3430/4000] [Steps  117] [reward 118.0]\n",
    "# [Episode 3440/4000] [Steps   10] [reward 11.0]\n",
    "# [Episode 3450/4000] [Steps   91] [reward 92.0]\n",
    "# ----------\n",
    "# [TEST Episode 3450] [Average Reward 115.5]\n",
    "# ----------\n",
    "# [Episode 3460/4000] [Steps   79] [reward 80.0]\n",
    "# [Episode 3470/4000] [Steps   28] [reward 29.0]\n",
    "# ----------\n",
    "# [TEST Episode 3475] [Average Reward 119.9]\n",
    "# ----------\n",
    "# [Episode 3480/4000] [Steps  129] [reward 130.0]\n",
    "# [Episode 3490/4000] [Steps  100] [reward 101.0]\n",
    "# [Episode 3500/4000] [Steps  139] [reward 140.0]\n",
    "# ----------\n",
    "# [TEST Episode 3500] [Average Reward 115.8]\n",
    "# ----------\n",
    "# [Episode 3510/4000] [Steps   76] [reward 77.0]\n",
    "# [Episode 3520/4000] [Steps    9] [reward 10.0]\n",
    "# ----------\n",
    "# [TEST Episode 3525] [Average Reward 152.6]\n",
    "# ----------\n",
    "# [Episode 3530/4000] [Steps  226] [reward 227.0]\n",
    "# [Episode 3540/4000] [Steps   32] [reward 33.0]\n",
    "# [Episode 3550/4000] [Steps   87] [reward 88.0]\n",
    "# ----------\n",
    "# [TEST Episode 3550] [Average Reward 75.5]\n",
    "# ----------\n",
    "# [Episode 3560/4000] [Steps   17] [reward 18.0]\n",
    "# [Episode 3570/4000] [Steps  156] [reward 157.0]\n",
    "# ----------\n",
    "# [TEST Episode 3575] [Average Reward 377.8]\n",
    "# ----------\n",
    "# [Episode 3580/4000] [Steps  241] [reward 242.0]\n",
    "# [Episode 3590/4000] [Steps  142] [reward 143.0]\n",
    "# [Episode 3600/4000] [Steps   45] [reward 46.0]\n",
    "# ----------\n",
    "# [TEST Episode 3600] [Average Reward 163.3]\n",
    "# ----------\n",
    "# [Episode 3610/4000] [Steps  140] [reward 141.0]\n",
    "# [Episode 3620/4000] [Steps  102] [reward 103.0]\n",
    "# ----------\n",
    "# [TEST Episode 3625] [Average Reward 112.0]\n",
    "# ----------\n",
    "# [Episode 3630/4000] [Steps   86] [reward 87.0]\n",
    "# [Episode 3640/4000] [Steps   82] [reward 83.0]\n",
    "# [Episode 3650/4000] [Steps   12] [reward 13.0]\n",
    "# ----------\n",
    "# [TEST Episode 3650] [Average Reward 89.6]\n",
    "# ----------\n",
    "# [Episode 3660/4000] [Steps   89] [reward 90.0]\n",
    "# [Episode 3670/4000] [Steps   20] [reward 21.0]\n",
    "# ----------\n",
    "# [TEST Episode 3675] [Average Reward 500.0]\n",
    "# ----------\n",
    "# [Episode 3680/4000] [Steps   10] [reward 11.0]\n",
    "# [Episode 3690/4000] [Steps  118] [reward 119.0]\n",
    "# [Episode 3700/4000] [Steps   96] [reward 97.0]\n",
    "# ----------\n",
    "# [TEST Episode 3700] [Average Reward 104.3]\n",
    "# ----------\n",
    "# [Episode 3710/4000] [Steps  128] [reward 129.0]\n",
    "# [Episode 3720/4000] [Steps  108] [reward 109.0]\n",
    "# ----------\n",
    "# [TEST Episode 3725] [Average Reward 125.4]\n",
    "# ----------\n",
    "# [Episode 3730/4000] [Steps  116] [reward 117.0]\n",
    "# [Episode 3740/4000] [Steps   10] [reward 11.0]\n",
    "# [Episode 3750/4000] [Steps   99] [reward 100.0]\n",
    "# ----------\n",
    "# [TEST Episode 3750] [Average Reward 92.6]\n",
    "# ----------\n",
    "# [Episode 3760/4000] [Steps   74] [reward 75.0]\n",
    "# [Episode 3770/4000] [Steps  120] [reward 121.0]\n",
    "# ----------\n",
    "# [TEST Episode 3775] [Average Reward 18.2]\n",
    "# ----------\n",
    "# [Episode 3780/4000] [Steps  499] [reward 500.0]\n",
    "# [Episode 3790/4000] [Steps   86] [reward 87.0]\n",
    "# [Episode 3800/4000] [Steps   16] [reward 17.0]\n",
    "# ----------\n",
    "# [TEST Episode 3800] [Average Reward 51.4]\n",
    "# ----------\n",
    "# [Episode 3810/4000] [Steps   87] [reward 88.0]\n",
    "# [Episode 3820/4000] [Steps   62] [reward 63.0]\n",
    "# ----------\n",
    "# [TEST Episode 3825] [Average Reward 63.4]\n",
    "# ----------\n",
    "# [Episode 3830/4000] [Steps   12] [reward 13.0]\n",
    "# [Episode 3840/4000] [Steps  126] [reward 127.0]\n",
    "# [Episode 3850/4000] [Steps   42] [reward 43.0]\n",
    "# ----------\n",
    "# [TEST Episode 3850] [Average Reward 55.5]\n",
    "# ----------\n",
    "# [Episode 3860/4000] [Steps  154] [reward 155.0]\n",
    "# [Episode 3870/4000] [Steps   63] [reward 64.0]\n",
    "# ----------\n",
    "# [TEST Episode 3875] [Average Reward 128.0]\n",
    "# ----------\n",
    "# [Episode 3880/4000] [Steps  114] [reward 115.0]\n",
    "# [Episode 3890/4000] [Steps   90] [reward 91.0]\n",
    "# [Episode 3900/4000] [Steps  163] [reward 164.0]\n",
    "# ----------\n",
    "# [TEST Episode 3900] [Average Reward 142.5]\n",
    "# ----------\n",
    "# [Episode 3910/4000] [Steps   18] [reward 19.0]\n",
    "# [Episode 3920/4000] [Steps  111] [reward 112.0]\n",
    "# ----------\n",
    "# [TEST Episode 3925] [Average Reward 10.9]\n",
    "# ----------\n",
    "# [Episode 3930/4000] [Steps    7] [reward 8.0]\n",
    "# [Episode 3940/4000] [Steps   91] [reward 92.0]\n",
    "# [Episode 3950/4000] [Steps   68] [reward 69.0]\n",
    "# ----------\n",
    "# [TEST Episode 3950] [Average Reward 81.2]\n",
    "# ----------\n",
    "# [Episode 3960/4000] [Steps    7] [reward 8.0]\n",
    "# [Episode 3970/4000] [Steps   77] [reward 78.0]\n",
    "# ----------\n",
    "# [TEST Episode 3975] [Average Reward 127.1]\n",
    "# ----------\n",
    "# [Episode 3980/4000] [Steps  248] [reward 249.0]\n",
    "# [Episode 3990/4000] [Steps   14] [reward 15.0]\n",
    "# [Episode 4000/4000] [Steps   61] [reward 62.0]\n",
    "# ----------\n",
    "# [TEST Episode 4000] [Average Reward 41.9]\n",
    "# ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the replay buffer improve performance?\n",
    "\n",
    "**Answer:** The replay buffer improved the learning efficiency. It only took ~725 episodes to reach a good reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extra (fully optional)\n",
    "\n",
    "Ideas to experiment with:\n",
    "\n",
    "- Is $\\epsilon$-greedy strategy the best strategy available? Experiment with other strategies.\n",
    "- Make use of the model you have trained in the behavioral cloning part and fine-tune it with RL. How does that affect performance?\n",
    "- You are perhaps bored with `CartPole-v1` by now. Another environment we suggest trying is `LunarLander-v2`. It will be harder to learn but with experimentation, you will find the correct optimizations for success. Piazza is also your friend :)\n",
    "- What about learning from images? This requires more work because you have to extract the image from the environment. How much more challenging might you expect the learning to be in this case?\n",
    "- An improvement over DQN is DoubleDQN. Experiment with this to see how much of an impact it makes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU CAN USE THIS CODEBLOCK AND ADD ANY BLOCK BELOW AS YOU NEED\n",
    "# TO SHOW US THE IDEAS AND EXTRA EXPERIMENTS YOU RUN.\n",
    "# HAVE FUN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
